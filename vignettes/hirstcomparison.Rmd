---
title: "Comparison of Pollen Traps"
subtitle: "Evaluation of Similarity and Robustness of Three Hirst-type Pollen Traps Located in Payerne During the Blooming Season 2013"
author: "Simon Adamov"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warnings = FALSE, 
                      message = FALSE,
                      fig.retina =4,
                      fig.width = 10,
                      fig.height = 6,
                      out.width = "100%",
                      out.height = "100%")
# This project is using renv dependency management, for more info: https://cran.r-project.org/web/packages/renv/vignettes/renv.html

library(caTools)
library(MASS)
library(tidyverse)
library(lubridate)
library(ggpubr)
library(here)
library(lme4)
library(psych)
library(robustlmm)
library(goftest)
library(kableExtra)

# library(Amelia) # only used for one fct call to missmap()
# library(data.table) # only used for transpose()

devtools::load_all()

# I like the look of these plots
# devtools::install_github('cttobin/ggthemr')
ggthemr::ggthemr("fresh")

# caTools and bitops are required to knit this vignette, those are not tracked by renv and must be installed by the user
# Due to old R-Version caTools must be installed from CRAN Archive, in order to knit markdown documents.
# packageurl <- "http://cran.r-project.org/src/contrib/Archive/caTools/caTools_1.17.1.1.tar.gz"
# install.packages(packageurl, repos=NULL, type="source")

```
Bioaerosols have important impacts on human health (EACCI, 2015; Pawankar et al., 2013) and agriculture (Cunha et al. 2016; Oteros et al. 2014), and serve as important indicators for identification of invasive species (Karrer et al., 2015; Sikoparija et al., 2009) and climate change (Zhang et al. 2015; Ziello et al. 2012). As such, networks to monitor these particles, particularly pollen and spores, have been established in many countries over the past decades (Hertel et al., 2013). At present, the majority of these networks employ manual methods based on Hirst-type volumetric samplers (Hirst, 1952) for monitoring purposes. Air sucked is drawn through these devices and particles impact onto a rotating tape, which is usually collected and analysed using light microscopy manually on a weekly basis.

Standards to ensure comparability within and across networks have been established for routine monitoring of airborne pollen and spores (Galan et al. 2014; CEN/TS 16868, 2015). Nevertheless, these methods are known to suffer from a number of shortcomings, including collection efficiency onto the rotating tape (Orlandi et al,. 2014; Mandrioli et al., 1998),  errors in estimates of the flowrate used to calculate concentrations (Oteros et al., 2017), sampling efficiency (Sikoparija et al., 2011; Tormo-Molina et al., 1996; Käpylä and Penttinen 1981),  issues related to the manual counts (Rojo, 2019; Comptois et al., 1999; Pedersen and Moseholm, 1993), and quality control (Sikoparija et al., 2016; Berti et al. 2009). 

While several studies have compared measurements from separate devices of the same kind, they have rarely been relatively few comparisons of more than two devices and often these have been at considerable distance from one another (e.g. Irdi et al., 2002; Gottardini and Cristofolini, 1997; Ekebom et al., 1997; Rizzi et al., 1992; Meiffren, 1988).  Molina et al. 2013 compared five traps at the same site and found significant differences in terms of the timing of peak pollen concentrations when hourly data were considered, however, no such differences were evident in the analysis of daily data. In contrast, Rojo et al. 2019 compared three devices in Munich, Germany, and two traps in Zrenjanin, Serbia, and found average standard deviations of 34% between Hirst-type traps, even when daily averages were considered. Although ratios of mean pollen concentrations between the instruments were mostly close to 1, they found that the standard deviations of the pollen concentrations varied significantly on the daily scale; a possible explanation for why no significant differences in previous work (Rojo et al., 2019). 

Fully understanding all source of errors related to traditional manual measurements is particularly important given the advent of automatic pollen monitors based on new technologies such as image-recognition (Oteros et al., 2019?) and air-flow cytometry (Sauvageat et al., 2019; Crouzy et al., 2016). For validation purposes, these new, real-time instruments are compared against traditional, manual observations and it is thus essential to understand what level of error can be expected from the various measurement techniques to make robust comparisons. 

In this study we compare measurements from three Hirst-type traps placed side-by-side in Payerne, Switzerland, from the pollen season of 2013. While these observations date back somewhat, counting methods have not changed significantly since and the data remain valid. We aim to investigate more fully the variability between measurements at the same location at both the daily and hourly scale. 

# Data

Three Hirst-type traps (Burkard Manufacturing, UK) were co-located on the roof of the Federal Office of Meteorology and Climatology MeteoSwiss building in Payerne, Switzerland. The roof is flat and at a height of 7m. The Hirst-type traps were placed in a line, approximately 3m from each other. The study period ran from 26 March to 25 July 2013 and the three Hirst-type traps operated simultaneously for this entire period.

The three Hirst-type traps (numbered 2, 4, and 8) were carefully calibrated and set for seven-day sampling using Melinex tape coated with silicon (Lanzoni, Italy). The volume of air sampled was checked each week using a flowmeter when the drums were exchanged. Tapes were cut into seven daily segments and mounted onto microscope slides. Two longitudinal scans were counted on each slide using an Olympus microscope with a 600x magnification and, at a later stage, two additional lines were counted at 400x magnification. An average of all four lines was taken to calculate the final concentrations. P Data are expressed as daily average concentrations (pollen grains/m3). 
All pollen types were counted and included in the “total pollen” category. We also selected 20 individual pollen types for a more precise comparison: Urtica, Castanea, Salix, Platanus, Betula, Alnus, Corylus, Fraxinus, Rumex, Taxus, Cupressus, Poaceae, Ulmus, Populus, Plantago, Quercus, Fagus Juglans Carpinus, Picea, and Pinus. This list aims to include the most important pollen types measured during the 2013 campaign in Payerne and to cover wide range of pollen sizes to provide a better understanding of how pollen counts vary with grain size. 

## Import

```{r}

# The data was provided by Fiona Tummon, in form of 10-minute pollen counts.
# The data prep until that point was not re-evaluated.
# The datetimes were provided in a seperate file (in local time)
# PAY2, PAY4 and PAY8 are three different pollen traps
# lines 2,4,6,7 are four different lines, along which the pollen were manually counted under the microsope.
# Lines 2 and 6 were counted in 2013, whereas lines 4 and 7 were counted in 2019

path_data <- paste0(here::here(), "/ext-data/")
files_data <- list.files(path_data, pattern = "2013_10min.csv")
files_datetimes <- list.files(path_data, pattern = "dates_10min.csv")

data_raw <-
  map(files_data, ~ read_delim(
    paste0(path_data, .x),
    delim = ";",
    col_names = TRUE,
    col_types = cols(.default = "n"),
    na = "nan"
    )
  ) %>% 
  setNames(files_data)

dates_raw <-
  map(files_datetimes,~ read_delim(
    paste0(path_data, .x),
    delim = ";",
    col_names = FALSE,
    col_types = cols(.default = "n") # dates and times are in a weird format, have to import as numeric as a consequence
    )
  ) %>% 
  setNames(files_datetimes)

dates <- map(dates_raw, ~mutate(
  .x,
  time = sprintf("%04d", as.integer(X2)), # otherwise we lose leading zeros
  datetime = ymd_hm(paste0(X1, time)),
  datetime_adj = datetime - minutes(10), # This is necessary to cope with the guidelines mentioned below
  hour = hour(datetime_adj) + 1,
  date = date(datetime_adj)) %>% 
  select(datetime, date, hour)
  )

# Combining the measurements with the datetimes for the three traps
# It appears that the traps have slightly different observations (e.g. 9 minutes past the full hour instead of 10). This will have no impact, once the concentrations are averaged per hour
data <- map2(data_raw, rep(1:3, each = 4), ~bind_cols(.x, dates[[.y]]) %>% mutate(trap = 2^(.y)))

# Create a new variable to differentiate between counted lines; note that traps were already defined in the previous map-call above
data <- map2(data, rep(c(2, 4, 6, 7), 3), ~mutate(.x, line = .y))

# We are only interested on a subset of the data. The other species either have very low measurements in Switzerland, or they are not relevant from a medical point of view. This selection has been discussed with Regula Gehrig and has been used for a longer time now, is my understanding.
# The order of variables is different in the PAY2 and PAY6 data, hence we quicly sort them here:
species_selection <- c("Castanea", "Alnus", "Ulmus", "Cupressus", "Fraxinus", "Fagus", "Juglans", "Plantago", "Corylus", 
    "Pinus", "Quercus", "Rumex", "Platanus", "Populus", "Poaceae", "Salix", "Betula", "Carpinus", 
    "Urtica", "Taxus", "Picea")

data <- map(data, ~select(.x, 
    kacastz0, kaalnuz0, kaulmuz0, kacuprz0, kafraxz0, kafaguz0, kajuglz0, khplanz0, kacoryz0, 
    kapinuz0, kaquerz0, khrumez0, kaplatz0, kapopuz0, khpoacz0, kasaliz0, kabetuz0, kacarpz0, 
    khurtiz0, kataxuz0, kapicez0, datetime, date, hour, trap, line) %>% 
  setNames(c(species_selection, "datetime", "date", "hour", "trap", "line")))

```

## Data Bias Correction

The team in Payerne found that the air sucking rates of the Hirst traps are actually higher in reality than reported by the manufacturer.
The traps suck in 13.5 l per minute instead of 10 l per minute. Hence our Pollen counts per 10 minutes are too high and should be reduced by a factor of 1.35.

```{r}
data <- map(data, ~mutate_at(.x, vars(all_of(species_selection)), ~./1.35))
```

There are a few missing measurements, we will impute/exclude those in the next step.
Generally, if one species is NA in one specific trap/line at a certain time, then all species are NA.
Hence we could assume that the traps had a malfunction during that time.
What is surprising though is the fact that not all lines have the same amount of NAs.
This might suggest that some entries were omitted during the manual pollen counting exercise.
If these values are outside of the blooming season then we don't have an issue. This should be further evaluated.

## Imputation and Timeframe Selection

There are some periods where the traps did not measure (possibly malfunction?). Those can be observed in the plot below, for trap 2 and line 2.

```{r}
# Later measurements are NA for all traps and hence can be removed, to make sure that all traps and lines have the same amount of rows.
# map(data, ~Amelia::missmap(.x, y.labels = "", y.at = 1, col = ggthemr::swatch()[c(4,2)]))
data[[1]] %>% as.data.frame() %>% Amelia::missmap(y.labels = "", y.at = 1, col = ggthemr::swatch()[c(4,2)])

```

We will exclude all observations where the measurement of all lines and traps is lower than 10 Pollen/m^3, as discussed with Regula Gehrig. We are mostly interested in observations where Pollen are actually being measured and not the ones where no trap detected anything (which is usually true for many days outside the blooming season). This will lead to a large increase in NA values (that will not be plotted or used for statistical comparison).

```{r}
# Make sure that all dataframes have the same amount of rows, working with max amount of data available
# It's probably better to work with full days only, so that each measurement independet of the aggregation window has the same meaning
data <- map(data, ~.x %>% filter(datetime <= as_datetime("2013-06-30 00:00:00"),
                                 datetime >= as_datetime("2013-04-02 00:10:00"))) 


# data <- map(data, ~.x %>% replace(is.na(.), 0)) # NAs are currently not imputed, there are some missing values though. In the functions below we remove NA whily executing summarising functions (e.g. sum(x, na.rm = TRUE))

# We will exclude all observations where the measurement of all lines and traps is lower than 10 Pollen/m^3, as discussed with Regula Gehrig
# We are mostly interested in observations where Pollen are actually being measured and not the ones where no trap detected anything (which is usually true for many days outside the blooming season).
data <- data %>% 
  bind_rows %>%   
  group_by(datetime) %>% 
  mutate_at(vars(all_of(species_selection)), function(x){
      if(sum(x, na.rm = TRUE) > 0) 
        return(x)
      else 
        return(NA_real_)
  }) %>% 
  ungroup %>% 
  group_split(trap,line)

data[[1]] %>% as.data.frame() %>% Amelia::missmap(y.labels = "", y.at = 1, col = ggthemr::swatch()[c(4,2)])

```

## Aggregation

### By Species

Five concentration classes were defined for the analysis: 0-10 pollen grains/m3, 10-20 pollen grains/m3, 20-50 pollen grains/m3, 50-100 pollen grains/m3, 100-300 grains/m3, and >300 pollen grains/m3, with the aim of better understanding whether different pollen concentrations have an impact on sampling and measurement error.
Furthermore, to investigate the impact of pollen size on sampling and measurement error, five groups of pollen were defined according to their average sizes (Beug, XXXX):

- Group 1: Urtica, Castanea
- Group 2: Alnus, Betula, Corylus, Fraxinus, Platanus, Rumex, Salix
- Group 3: Plantago, Poaceae, Populus, Taxus, Cupressus, Ulmus
- Group 4: Carpinus, Fagus, Juglans, Quercus
- Group 5: Pinus, Picea

```{r}

Group10_20 <- c("Urtica", "Castanea")
Group20_50 <- c("Alnus", "Betula", "Corylus", "Fraxinus", "Platanus", "Rumex", "Salix")
Group50_100 <- c("Plantago", "Poaceae", "Populus", "Taxus", "Cupressus", "Ulmus")
Group100_300 <- c("Carpinus", "Fagus", "Juglans", "Quercus")
Group300 <- c("Pinus", "Picea")

data <- map(data, ~.x %>% 
  mutate(Group10_20 = if_else(!is.na(Urtica) | !is.na(Castanea), 
                              rowSums(.[names(.x) %in% Group10_20], na.rm = TRUE), 
                              NA_real_),
         Group20_50 = if_else(!is.na(Alnus) | !is.na(Betula) | !is.na(Corylus) | !is.na(Fraxinus) | !is.na(Platanus) | !is.na(Rumex) | !is.na(Salix), 
                              rowSums(.[names(.x) %in% Group20_50], na.rm = TRUE), 
                              NA_real_),
         Group50_100 = if_else(!is.na(Plantago) | !is.na(Poaceae) | !is.na(Populus) | !is.na(Taxus) | !is.na(Cupressus) | !is.na(Ulmus), 
                               rowSums(.[names(.x) %in% Group50_100], na.rm = TRUE), 
                               NA_real_),
         Group100_300 = if_else(!is.na(Carpinus) | !is.na(Fagus) | !is.na(Juglans) | !is.na(Quercus), 
                                rowSums(.[names(.x) %in% Group100_300], na.rm = TRUE), 
                                NA_real_),
         Group300 = if_else(!is.na(Pinus) | !is.na(Picea),
                                rowSums(.[names(.x) %in% Group300], na.rm = TRUE), 
                                NA_real_),
         Total = if_else(!is.na(Group10_20) | !is.na(Group20_50) | !is.na(Group50_100) | !is.na(Group100_300) | !is.na(Group300), 
                         rowSums(.[names(.x) %in% species_selection], na.rm = TRUE), 
                         NA_real_)))

```


### Temporal

One of the questions is, how long the observation period should be for the Pollen traps to produce robust results on any specific day.
The original time span is 10minutes, we want to look at various average concentration (e.g. hourly, 3 hours, 6 hours, daily).

There are some general guidelines from Regula Gehrig on how to average concentration that should be followed here.

Die Stundenwerte werden wie folgt aus den 10-Minutenwerten berechnet:
z.B. Wert für 6:00 Uhr UTC wird berechnet aus 05:10 – 6:00 UTC

Die 3-h-Werte, die du auch im DWH findest, werden so berechnet:
z.B. Wert um 09:00 UTC: 06:10-09:00

Die Tageswerte werden bei den Pollen glaub ich aus den Stundenwerten berechnet aus:
01:00 – 24:00 (d.h. 00:00 – im DWH gibt es keinen Wert 24:00) UTC 

```{r}

# Hour 1 represents the duration 00:10 - 01:00, and same for all hours up to 24
data_hourly <- map(data, ~.x %>% 
  group_by(date, hour, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~mean(., na.rm = TRUE)) %>% 
  ungroup())


# # The data comes in a sorted format by date and time, below I check that the start and endpoint is the same when averaged hourly
# # This is actually the case with the first measurement on 2013-04-01 11 o'clock and the last on 2013-07-01 - 10 o'clock (as trimmed above).
# # All lines and traps show the same observations.
# map(data_hourly, ~.x %>%
#   slice(c(1, nrow(.x))) %>%
#   rowid_to_column) %>%
#   bind_rows() %>%
#   arrange(rowid)
# # All the traps have the same amount of unique observations
# map(data_hourly, ~.x %>% select(date, hour) %>% unique %>% nrow)
# map(data_hourly, ~.x %>% slice(tail(row_number(), 10)))

data_hours3 <- map(data, ~.x %>% 
  mutate(hours3 = case_when(
    hour >= 1 & hour < 3 ~ "06:00",
    hour >= 3 & hour < 6 ~ "12:00",
    hour >= 6 & hour < 9 ~ "18:00",
    hour >= 9 & hour < 12 ~ "24:00",
    hour >= 12 & hour < 15 ~ "06:00",
    hour >= 15 & hour < 18 ~ "12:00",
    hour >= 18 & hour < 21 ~ "18:00",
    hour >= 21 & hour < 24 ~ "24:00"
    )
  ) %>% 
  group_by(date, hours3, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~mean(., na.rm = TRUE)) %>% 
  ungroup())

data_hours6 <- map(data, ~.x %>% 
  mutate(hours6 = case_when(
    hour >= 1 & hour <= 6 ~ "06:00",
    hour > 6 & hour <= 12 ~ "12:00",
    hour > 12 & hour <= 18 ~ "18:00",
    hour > 18 & hour <= 24 ~ "24:00"
    )
  ) %>% 
  group_by(date, hours6, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~mean(., na.rm = TRUE)) %>% 
  ungroup())

data_hours12 <- map(data, ~.x %>% 
  mutate(hours12 = case_when(
    hour >= 1 & hour <= 12 ~ "12:00",
    hour > 12 & hour <= 24 ~ "24:00"
    )
  ) %>% 
  group_by(date, hours12, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~mean(., na.rm = TRUE)) %>% 
  ungroup())

data_daily <- map(data_hourly, ~.x %>% 
                    group_by(date, trap, line) %>% 
                    summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~mean(., na.rm = TRUE)) %>% 
                    ungroup())


```

# Tabular Comparison of Various Metrics

```{r}

occurence <- data_daily %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~sum(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~sum(. !=0, na.rm = TRUE)) %>% 
  ungroup() 

names <- names(occurence)

occurence <- occurence %>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  select(name, trap2, trap4, trap8) %>% 
  filter(name != "trap")

maximum <- data_daily %>% 
  bind_rows %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~max(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

average <- data_daily %>% 
  bind_rows %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~mean(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

spi <- data_daily %>% # Not sure if this daily sume is correct
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~sum(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)


occurence %>% 
  inner_join(maximum, by = "name") %>% 
  inner_join(average, by = "name") %>% 
  inner_join(spi, by = "name") %>% 
  filter(name %in% species_selection) %>% 
  arrange(desc(trap8.y.y)) %>% 
  setNames(c("Name", rep(paste0("trap", c(2, 4, 8)), times = 4))) %>% 
  kable(escape = FALSE) %>%
  kable_styling(c("striped", "condensed"), full_width = FALSE) %>% 
  add_header_above(c(" " = 1, "Frequency of Occurence" = 3, "Maximum Daily Concentration" = 3, "Average Daily Concentration" = 3, "Seasonal Pollen Integral" = 3)) %>% 
  column_spec(1, italic = TRUE) %>% 
  column_spec(2:13, width = "2.5cm")

```


# Visualization

Now let's plot some time series to get a better overview of the data. In the plot below the user can select any species and temporal resolution to compare the measurements from different lines and traps.

```{r }

resolution <- "6hour"
species <- "Total"

plot <- plot_pollen(species = species, resolution = resolution, group = "trap", traps = c(2, 4, 8), rm_zeros = TRUE)
plot
# ggsave(filename = "plot.png", plot = plot, width = 8, height = 4.5)

```


# Statistical Comparison

The analysis should also serve as a base for a scientific publication. We want to assess the robustness and similarity of the Hirst traps. For that purpose we will compare the measured Pollen concentrations for the three different traps and also the four lines. Thereby, we vary:

- The temporal resolution for the calculation of the mean concentrations
- Buckets of Pollen concentrations, to distinguish between high and low blooming season
- The threshold below which Pollen measurements are set to NA (because they become unreliable)

In addition Several metrics were calculated: the frequency of occurrence for each pollen was obtained by counting the number of days that each particular pollen was detected; the seasonal mean was calculated by averaging values for each pollen for all the days that it occurred; and the Seasonal Pollen Index (SPI) was calculated by integrating the concentrations of a particular pollen over the entire season (Mandrioli et al., 1998). Average and maximum concentrations were also calculated, as well as the standard deviations for both daily and hourly values.

There are two pathways to evaluate the similarity of the measurements between the traps (measurers). One can either transform the measurements (log), so that they are normally distributed and then work with ANOVA, F-Test and Tukey Honestly Significance Difference (HSD); or one can take the measurements as is (counts per m³) and work with the non-parametric Kruskal-Vallis Test that only requires rank-symmetry but doesn't assume any underlying distribution of the data. For the non-parametric contrats multiple options present themselves that we will investigate below.

```{r}

if (resolution == "daily"){
  data_anova <- map(data_daily, ~.x %>%
                     mutate(timestamp = date))
} else if (resolution == "12hour"){
  data_anova <- map(data_hours12, ~.x %>%
                     mutate(timestamp = ymd_hm(paste0(date, hours12))))
} else if (resolution == "6hour"){
  data_anova <- map(data_hours6, ~.x %>%
                     mutate(timestamp = ymd_hm(paste0(date, hours6))))
} else if (resolution == "3hour"){
  data_anova <- map(data_hours3, ~.x %>%
                     mutate(timestamp = ymd_hm(paste0(date, hours3))))
} else if (resolution == "hourly"){
  data_anova <- map(data_hourly, ~.x %>%
                     mutate(timestamp = ymd_hm(paste0(date, paste0(sprintf("%02d", hour), ":00")))))
}

data_anova <- data_anova %>% 
  bind_rows() %>% 
  mutate(trap = as.factor(trap)) %>% 
  group_by(trap, timestamp) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~mean(., na.rm = TRUE)) %>% 
  ungroup 

data_transformed <- data_anova %>% 
  mutate_at(vars(all_of(species_selection), Total, Group10_20, Group20_50, Group50_100, Group100_300, Group300), ~log(. + 1))
  
# There are various methods to deal with zeros during log transformation
# 
#     Add a constant value © to each value of variable then take a log transformation - i will add x + 1
#     Impute zero value with mean. - not evaluated
#     Take square root instead of log for transformation - does not really help much

  
  
```


## Correlation

The correlation between the traps can be calculated easily and then the CI and p-values must be adjusted for multiple comparison.
The corr-test function from the psych handily offers this functionality.

### Pearson, Spearman and Kendall Correlation

```{r}

methods <- c("pearson", "spearman", "kendall")

data_corr <- data_transformed %>% # For the robust methods (spearman, kendall) it doesn't matter whether the transformed data is used or the original
  select(!!sym(species), trap, timestamp) %>% 
  pivot_wider(names_from = trap, values_from = !!sym(species), timestamp) %>% 
  setNames(c("timestamp", paste0("trap", c(2, 4, 8))))

corr_matrix <- map(methods, ~corr.test(
  data_corr %>% select(-timestamp),
  use = "complete",
  method = .x,
  adjust = "holm",
  alpha = .05,
  ci = TRUE,
  minlength = 5
  ))

# dummy <- map(corr_matrix, ~.x %>% print(short = FALSE))


```

```{r}

ci <- map(corr_matrix, ~.x %>% 
  pluck(10)) %>% 
  bind_rows() %>% 
  round(2) %>% 
  mutate(method = rep(methods, each = 3)) %>%  
  mutate(ci = paste0("R-", tools::toTitleCase(method), ": ", lower.adj, " - ", upper.adj)) %>% 
  pull(ci)

gg1 <- data_corr %>% 
  ggplot(aes(x = trap2, y = trap4)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(alpha = 0.2) +
  geom_abline(slope = 1, intercept = 0, col = ggthemr::swatch()[4]) +
  geom_label(label = c(paste(ci[1], "\n", ci[4], "\n", ci[7])), aes(x = 3.6, y = 9)) +
  coord_cartesian(ylim = c(0, 10), xlim = c(0, 10))

gg2 <- data_corr %>% 
  ggplot(aes(x = trap2, y = trap8)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(alpha = 0.2) +
  geom_abline(slope = 1, intercept = 0, col = ggthemr::swatch()[4]) +
  geom_label(label = c(paste(ci[2], "\n", ci[5], "\n", ci[8])), aes(x = 3.6, y = 9)) +
  coord_cartesian(ylim = c(0, 10), xlim = c(0, 10))

gg3 <- data_corr %>% 
  ggplot(aes(x = trap4, y = trap8)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(alpha = 0.2) +
  geom_abline(slope = 1, intercept = 0, col = ggthemr::swatch()[4]) +
  geom_label(label = c(paste(ci[3], "\n", ci[6], "\n", ci[9])), aes(x = 3.6, y = 9)) +
  coord_cartesian(ylim = c(0, 10), xlim = c(0, 10))

title <- tools::toTitleCase(paste0("Comparison of ", resolution, " average concentrations of ", species, " pollen between the three traps"))

ggarrange(gg1, gg2, gg3, ncol = 3) %>%
  annotate_figure( top = title, bottom = text_grob("Pairwise correlation between traps; grey line shows the Loess smother; the red line shows a theroratical perfect correlation of 1. \n In the text box one can see the 95% confidence intervals of the R-values (adjusted for multiple comparison) as obtained by Pearson and two robust methods.", color = ggthemr::swatch()[1], face = "italic", size = 10))
```

## Parametric / ANOVA

First we compare the traps with the traditional ANOVA approach. Statistical inference (p-values, confidence intervals, . . . ) is only valid if the model assumptions are fulfilled. So far, this means (many paragraphs are quoted from Lukas Meier ETH - Script Applied Statistics ANOVA Course):

- are the errors independent?
- are the errors normally distributed?
- is the error variance constant?
- do the errors have mean zero?

The first assumption is most crucial (but also most difficult to check). If the independence assumption is
violated, statistical inference can be very inaccurate. In the ANOVA setting, the last assumption is typically
not as important compared to a regression setting, as we are typically fitting “large” models.

Below we prepare the data averaging between the four lines in each trap. As we will see further below in the residuals section, it is necessary to logarithmise the measured concentrations.

### F-Test / Omnibus-Test

We first test for an overall significance of the trap on the measurements.

```{r}
fit_anova <- aov(as.formula(paste(species, "~ trap")), data = data_transformed, contrasts = c("contr.sum", "contr.poly"))

summary(fit_anova)
confint(fit_anova)
summary.lm(fit_anova)

```

### Residual Analysis

As mentioned above (and already applied), the residuals of the anova fit need to fulfil some assumptions that we want to check in the following.

- are the errors normally distributed?

In a QQ-plot we plot the empirical quantiles (“what we see in the data”) vs. the theoretical quantiles (“what
we expect from the model”). The plot should show a more or less straight line if the distributional assumption
is correct. By default, a standard normal distribution is the theoretical “reference distribution”.

```{r}
fit_anova_raw <- aov(as.formula(paste(species, "~ trap")), data = data_anova, contrasts = c("contr.sum", "contr.poly"))

tibble(residuals = residuals(fit_anova_raw, type = "pearson")) %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line()
```

They are definitely not and we have to do some adjustments. So for the folowing plot we logarithmised the data to deal with the right-skewedness. The best results were achieved by first logarithmising the data and then taking the square root.

```{r}
tibble(residuals = residuals(fit_anova, type = "pearson")) %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line()
```


- is the error variance constant?
- do the errors have mean zero?

The Tukey-Anscombe plot plots the residuals rij vs. the fitted values bμi. It allows us to check whether
the residuals have constant variance and whether the residuals have mean zero (i.e. they don’t show any
deterministic pattern). We don't plot the smoothing line as loess (and other) algorithms have issuess when the same value is repeated a large number of times (jitter did not really help).

```{r}
# plot(fit_anova, which = 1)

tibble(resid = residuals(fit_anova_raw, type = "pearson"), fitted = fit_anova_raw$fitted.values) %>%
  ggplot(aes(x = fitted, y = resid)) +
  geom_point(alpha = 0.5) +
  # geom_smooth(method = "loess", col = ggthemr::swatch()[4]) + # We need to add some jitter, because the same concentrations are repeated a larger number of time: https://stackoverflow.com/questions/38864458/loess-warnings-errors-related-to-span-in-r but it does not really help too much, i will use the base plot for now.
  geom_abline(slope = 0, intercept = 0, col = ggthemr::swatch()[3]) +
  coord_cartesian(ylim = c(-500, 500))

```

```{r}
# plot(fit_anova, which = 1)

tibble(resid = residuals(fit_anova, type = "pearson"), fitted = fit_anova$fitted.values) %>%
  ggplot(aes(x = fitted, y = resid)) +
  geom_point(alpha = 0.5) +
  # geom_smooth(method = "loess", col = ggthemr::swatch()[4]) + # We need to add some jitter, because the same concentrations are repeated a larger number of time: https://stackoverflow.com/questions/38864458/loess-warnings-errors-related-to-span-in-r but it does not really help too much, i will use the base plot for now.
  geom_abline(slope = 0, intercept = 0, col = ggthemr::swatch()[3])

```

- are the errors independent?

If the data has some serial structure (i.e., if observations were recorded in a certain time order), we typically
want to check whether residuals close in time are more similar than residuals far apart, as this would be a
violation of the independence assumption. We can do so by using a so-called index plot where we plot the
residuals against time. For positively dependendent residuals we would see time periods where most residuals
have the same sign, while for negatively dependent residuals, the residuals would “jump” too often from
positive to negative compared to independent residuals.

```{r}

resid <- residuals(fit_anova_raw, type = "pearson")
resid_df <- tibble(resid = resid, id = as.numeric(names(resid)))

tibble(id = 1:nrow(data_anova), time = data_anova$timestamp) %>%
  left_join(resid_df, by = "id") %>% 
  ggplot(aes(x = time, y = resid)) +
  geom_point() +
  geom_line(alpha = 0.3)
```

```{r}

resid <- residuals(fit_anova, type = "pearson")
resid_df <- tibble(resid = resid, id = as.numeric(names(resid)))

tibble(id = 1:nrow(data_anova), time = data_anova$timestamp) %>%
  left_join(resid_df, by = "id") %>% 
  ggplot(aes(x = time, y = resid)) +
  geom_point() +
  geom_line(alpha = 0.3)
```

### Contrasts / Tukey HSD

The F-test is rather unspecific. It basically gives us a “Yes/No” answer for the question “is there any
treatment effect at all?”. It doesn’t tell us what specific treatment (or treatment combination) is significant.
Quite often we have a more specific question than the aforementioned global null hypothesis. E.g., we might
want to compare a set of new treatments vs. a control treatment or we want to do pairwise comparisons
between many (or all) treatments.
Multiple Testing: The problem with all statistical tests is the fact that the (overall) error rate increases with increasing number
of tests.

```{r}
hsd <- TukeyHSD(fit_anova)
plot(hsd)
```

### Random Effects Model

So far we have been looking at three traps in isolation. But they are part of a larger "population" of Hirst traps being used at MCH or worldwide. Hence we also should look at ANOVA with trap as a random effect. This needs a bit more data per trap, otherwise the model fitting does not reach convergence and produce a so-called singular fit (i.e. daily averages don't work).

```{r}
fit_lmer <- lmer(as.formula(paste(species, "~ (1 | trap)")), data = data_transformed)
isSingular(fit_lmer) # This has to be fixed!
summary(fit_lmer)
confint(fit_lmer, oldNames = FALSE)
ranef(fit_lmer)
confint(fit_anova)
```

```{r}
plot(fit_lmer, which = 1)
```

```{r}
par(mfrow = c(1, 2))
qqnorm(ranef(fit_lmer)$trap[, "(Intercept)"], main = "Random effects")
qqnorm(resid(fit_lmer), main = "Residuals")
```

## Non-Parametric / Rank-Based Symmetrical

### Kruskal-Wallis Test

Kruskal-Wallis test by rank is a non-parametric alternative to one-way ANOVA test, which extends the two-samples Wilcoxon test in the situation where there are more than two groups. It’s recommended when the assumptions of one-way ANOVA test are not met. This tutorial describes how to compute Kruskal-Wallis test in R software. (http://www.sthda.com/english/wiki/kruskal-wallis-test-in-r)

Assumptions

The assumptions of the Kruskal-Wallis test are similar to those for the Wilcoxon-Mann-Whitney test.

- Samples are random samples, or allocation to treatment group is random. 
- The two samples are mutually independent. 
- The measurement scale is at least ordinal, and the variable is continuous. 
- If the test is used as a test of dominance, it has no distributional assumptions. If it used to compare medians, the distributions must be similar apart from their locations. 

The test is generally considered to be robust to ties. However, if ties are present they should not be concentrated together in one part of the distribution (they should have either a normal or uniform distribution)
https://influentialpoints.com/Training/kruskal-wallis_anova-principles-properties-assumptions.htm

The Wilcoxon signed-rank test assumes that the data are distributed symmetrically around the median. In other words, there should be roughly the same number of values above and below the median. This can be checked by visual inspection using histogram and density distribution.

```{r}
data_anova %>% 
  ggplot(aes(x = !!sym(species))) +
  geom_histogram(alpha = 0.7, bins = 100) +
  geom_freqpoly(col = ggthemr::swatch()[3], bins = 50) +
  # geom_density(col = ggthemr::swatch()[5]) +
  geom_vline(xintercept = median(data_anova %>% pull(!!sym(species)), na.rm = TRUE), col = ggthemr::swatch()[4])

```


```{r}
kruskal.test(as.formula(paste(species, "~ trap")), data = data_anova)
```

### Pairwise Wilcox

```{r}
pairwise.wilcox.test(data_anova %>% pull(!!sym(species)), 
                     data_anova$trap,
                     paired = FALSE,
                     p.adjust.method = "fdr",
                     conf.int = TRUE)
```

### Random Effects Model

```{r}
fit_rlmer <- rlmer(as.formula(paste(species, "~ (1 | trap)")), data = data_transformed)
summary(fit_rlmer)

fit_rlmer2 <- update(fit_rlmer, rho.sigma.e = psi2propII(smoothPsi, k = 1.5), rho.sigma.b = psi2propII(smoothPsi, k = 1.5))

compare(fit_lmer, fit_rlmer, fit_rlmer2, show.rho.functions = FALSE)

gg <- map(1:3, ~plot(fit_rlmer, which = .x))



```


### Dunn's test, the Conover-Iman test, and the Dwass-Steel-Crichtlow-Fligner

# Error / Residual Analysis

A second chapter of the paper should futher elaborate on the relative errors between the traps.
We are mostly interested to observe the relative error of one specific trap compared to the mean of the three traps at any given point in time. We will compare the relative error for different buckets of pollen concentrations, to see how errors behave when low amounts of pollen are measured. Furthermore, we would like to understand the shape of the error function and if possible fit a parametric curve to the relative differences from the mean.

```{r}
errors <- data_corr %>% 
  mutate(mean = (trap2 + trap4 + trap8) / 3) %>% 
  mutate_at(vars(paste0("trap", c(2, 4, 8))), ~(. - mean)) %>% 
  pivot_longer(cols = trap2:trap8, values_to = "error", names_to = "trap") %>% 
  mutate(error_rel = error / mean) %>% 
  select(-mean)

sd(errors$error, na.rm = TRUE)
sd(errors$error_rel, na.rm = TRUE)

x <- seq(-2.5, 2.5, length.out = 2000)
t_shape <- fitdistr(errors$error[!is.na(errors$error)], "t") # QRM::fit.st(errors$error) leads to same result without warning
m <- t_shape$estimate[1]
s <- t_shape$estimate[2]
df <- t_shape$estimate[3]
t_errors <- tibble(x = x, y = dt((x - m) / s, df = df) / s)

t_shape_rel <- fitdistr(errors$error_rel[!is.na(errors$error_rel)], "t") # QRM::fit.st(errors$error) leads to same result without warning
m_rel <- t_shape_rel$estimate[1]
s_rel <- t_shape_rel$estimate[2]
df_rel <- t_shape_rel$estimate[3]
t_errors_rel <- tibble(x = x, y = dt((x - m_rel) / s_rel, df = df_rel) / s_rel)

gg_t_abs <- errors %>%
  ggplot(aes(x=error, y = ..density..)) +
  geom_histogram(bins= 100) +
  geom_density(col = ggthemr::swatch()[4]) +
  geom_rug(aes(y = 0), position = position_jitter(height = 0), col = ggthemr::swatch()[5]) +
  geom_line(data = t_errors, aes(x = x, y = y), col = ggthemr::swatch()[6]) +
  coord_cartesian(xlim = c(-5, 5))

gg_t_rel <- errors %>%
  ggplot(aes(x=error_rel, y = ..density..)) +
  geom_histogram(bins= 100) +
  geom_density(col = ggthemr::swatch()[4]) +
  geom_rug(aes(y = 0), position = position_jitter(height = 0), col = ggthemr::swatch()[5]) +
  geom_line(data = t_errors_rel, aes(x = x, y = y), col = ggthemr::swatch()[6]) +
  coord_cartesian(xlim = c(-2, 2))

ggarrange(gg_t_abs, gg_t_rel, nrow = 2) %>% 
  annotate_figure(top = "Absolute/Relative Differences from the Average Measurements of the Three Traps",
                  bottom = text_grob("On top the absolute differences from the mean fitted with a density kernel estimator and a Student-t distribution. \n At the bottom the relative differences from the mean.", color = ggthemr::swatch()[1], face = "italic", size = 10))

```

```{r}

sample <- tibble(x = (x-m)/s, y = pt((x-m)/s, df = df))
sample_rel <- tibble(x = (x-m_rel)/s_rel, y = pt((x-m_rel)/s_rel, df = df))

errors %>% 
  ggplot(aes(x = error)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  coord_cartesian(xlim = c(-5, 5)) +
  geom_line(data = sample, aes(x = x, y = y))

errors %>% 
  ggplot(aes(x = error_rel)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  coord_cartesian(xlim = c(-5, 5)) +
  geom_line(data = sample_rel, aes(x = x, y = y))


```


```{r}
errors %>% 
  mutate(error_scaled = (error)) %>%
  ggplot(aes(sample = error_scaled)) +
  stat_qq(distribution = qt, dparams = as.list(df)) +
  stat_qq_line(distribution = qt, dparams = as.list(df))
```

```{r}
cvm.test((errors$error[!is.na(errors$error)] - m)/s, "pt", df = df, estimated = TRUE)
cvm.test((errors$error_rel[!is.na(errors$error_rel)] - m_rel)/s_rel, "pt", df = df_rel, estimated = TRUE)

ad.test((errors$error[!is.na(errors$error)] - m)/s, "pt", df = df, estimated = TRUE)
ad.test((errors$error_rel[!is.na(errors$error_rel)] - m_rel)/s_rel, "pt", df = df_rel, estimated = TRUE)

```


## For Different Pollen Concentration Groups

```{r}
# set.seed(123)
gg <- list()
cvm <- numeric()
ad <- numeric()

for (j in c("Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300", "Total")){
  errors <- data_transformed %>% 
    select(!!sym(j), trap, timestamp) %>% 
    pivot_wider(names_from = trap, values_from = !!sym(j), timestamp) %>% 
    setNames(c("timestamp", paste0("trap", c(2, 4, 8)))) %>% 
    mutate(mean = if_else(!is.na(trap2) | !is.na(trap4) | !is.na(trap8),
                          rowSums(.[2:4], na.rm = TRUE) / 3, 
                          NA_real_)) %>% 
    mutate_at(vars(paste0("trap", c(2, 4, 8))), ~(. - mean)) %>% 
    pivot_longer(cols = trap2:trap8, values_to = "error", names_to = "trap") %>% 
    select(-mean)
  
  sd <- sd(errors$error, na.rm = TRUE)
  t_shape <- QRM::fit.st(errors$error[!is.na(errors$error)]) # Actually lead to the same result as the bootstrapped version below
  # t_shape <- QRM::fit.mst(errors$error, method = "Brent", upper = 2, lower = 1, nit = 2000, tol = 1e-10) # https://magesblog.com/post/2013-03-12-how-to-use-optim-in-r/ or here https://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html
  m <- t_shape$par.ests[2]
  s <- t_shape$par.ests[3]
  df <- t_shape$par.ests[1]
  t_errors <- tibble(x = x, y = dt((x - m) / s, df = df) / s)
  
  # If the parameteres are estimated from the data then this test applies the method from Braun and splits the data into two equally sized subsets. Therefore the test results are not stable, especially for smaller datasets (coarse temporal resolution). I will try to bootstrap the testing and the use the  mean (not 100% if this is valid, tbd).
  
  for (i in 1:1000) {
    cvm_test <- cvm.test((errors$error[!is.na(errors$error)] - m)/s, "pt", df = df, estimated = TRUE)
    ad_test <- ad.test((errors$error[!is.na(errors$error)] - m)/s, "pt", df = df, estimated = TRUE)
    
    cvm[i] <- cvm_test$p.value
    ad[i] <- ad_test$p.value
  }
  
  cvm <- mean(cvm)
  ad <- mean(ad)
  
  gg[[j]] <- errors %>%
    ggplot(aes(x=error, y = ..density..)) +
    geom_histogram(bins= 100) +
    geom_density(col = ggthemr::swatch()[4]) +
    geom_rug(aes(y = 0), position = position_jitter(height = 0), col = ggthemr::swatch()[5]) +
    geom_line(data = t_errors, aes(x = x, y = y), col = ggthemr::swatch()[6]) +
    coord_cartesian(xlim = c(-2.5, 2.5)) +
    geom_label(label = paste("P-Values \n CVM Test:", round(cvm, 2), "\n AD-Test", round(ad, 2)), aes(x = -2, y = 2)) +
    geom_label(label = paste("SD:", round(sd, 3), "\n DF:", round(df, 2)), aes(x = 2, y = 2))
}

ggarrange(gg[[1]], gg[[2]], gg[[3]], gg[[4]], gg[[5]], gg[[6]], nrow = 3, ncol = 2)

```


# Some Thoughts

A short review of the methods applied in the draft paper so far. I would like to lay out why the methods used above are probably a preferred choice to compare the traps.

## Pairwise vs. Contrasts

Problem of multiple testing

## Non-parametric vs. parametric

Assumption of linearity is harder to achieve without adding much benefit (wilcox vs. t-test)

## linear regression vs. ordinary least products regression analysis 

Geometric mean regression, reduced major axis regression take into account that both y and x contain measurement errors.
If measurements have been made on a continuous scale, the main choice is between the Altman–Bland method of differ-
ences and least products regression analysis. It is argued that although the former is relatively simple to execute, it does not
distinguish adequately between fixed and proportional bias. Least products regression analysis, although more difficult to
execute, does achieve this goal. There is almost universal agreement among biostatisticians that the Pearson product–moment
correlation coefficient (r) is valueless as a test for bias.
http://www.jerrydallal.com/LHSP/compare.htm




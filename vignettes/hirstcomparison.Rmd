---
title: "Comparison of Pollen Traps"
subtitle: "Evaluation of Similarity and Robustness of Three Hirst-type Pollen Traps Located in Payerne During the Blooming Season 2013"
author: "Simon Adamov"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default

---

```{r}
##### PLEASE DEFINE SPECIES AND TEMPORAL RESOLTION HERE #####

species <- "Total"
resolution <- "daily" # What temporal resolution should be plotted c("daily", "12hour", "6hour", "3hour", "hourly")

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      error = FALSE, 
                      warning = FALSE, 
                      message = FALSE,
                      fig.retina =2.5,
                      fig.width = 10,
                      fig.height = 6,
                      out.width = "100%",
                      out.height = "100%")
# This project is using renv dependency management, for more info: https://cran.r-project.org/web/packages/renv/vignettes/renv.html

library(caTools)
library(MASS)
library(tidyverse)
library(lubridate)
library(ggpubr)
library(here)
library(lme4)
library(psych)
library(robustlmm)
library(goftest)
library(kableExtra)

# library(Amelia) # only used for one fct call to missmap()
# library(data.table) # only used for transpose()
# library(dunn.test)
# library(conover.test)

devtools::load_all()

# I like the look of these plots
# devtools::install_github('cttobin/ggthemr')
# library(ggthemr)
ggthemr("fresh")

# caTools and bitops are required to knit this vignette, those are not tracked by renv and must be installed by the user
# Due to old R-Version caTools must be installed from CRAN Archive, in order to knit markdown documents.
# packageurl <- "http://cran.r-project.org/src/contrib/Archive/caTools/caTools_1.17.1.1.tar.gz"
# install.packages(packageurl, repos=NULL, type="source")

```
Bio-aerosols have important impacts on human health (EACCI, 2015; Pawankar et al., 2013) and agriculture (Cunha et al. 2016; Oteros et al. 2014), and serve as important indicators for identification of invasive species (Karrer et al., 2015; Sikoparija et al., 2009) and climate change (Zhang et al. 2015; Ziello et al. 2012). As such, networks to monitor these particles, particularly pollen and spores, have been established in many countries over the past decades (Hertel et al., 2013). At present, the majority of these networks employ manual methods based on Hirst-type volumetric samplers (Hirst, 1952) for monitoring purposes. Air sucked is drawn through these devices and particles impact onto a rotating tape, which is usually collected and analysed using light microscopy manually on a weekly basis.

Standards to ensure comparability within and across networks have been established for routine monitoring of airborne pollen and spores (Galan et al. 2014; CEN/TS 16868, 2015). Nevertheless, these methods are known to suffer from a number of shortcomings, including collection efficiency onto the rotating tape (Orlandi et al,. 2014; Mandrioli et al., 1998),  errors in estimates of the flow-rate used to calculate concentrations (Oteros et al., 2017), sampling efficiency (Sikoparija et al., 2011; Tormo-Molina et al., 1996; Käpylä and Penttinen 1981),  issues related to the manual counts (Rojo, 2019; Comptois et al., 1999; Pedersen and Moseholm, 1993), and quality control (Sikoparija et al., 2016; Berti et al. 2009). 

While several studies have compared measurements from separate devices of the same kind, they have rarely been relatively few comparisons of more than two devices and often these have been at considerable distance from one another (e.g. Irdi et al., 2002; Gottardini and Cristofolini, 1997; Ekebom et al., 1997; Rizzi et al., 1992; Meiffren, 1988).  Molina et al. 2013 compared five traps at the same site and found significant differences in terms of the timing of peak pollen concentrations when hourly data were considered, however, no such differences were evident in the analysis of daily data. In contrast, Rojo et al. 2019 compared three devices in Munich, Germany, and two traps in Zrenjanin, Serbia, and found average standard deviations of 34% between Hirst-type traps, even when daily averages were considered. Although ratios of mean pollen concentrations between the instruments were mostly close to 1, they found that the standard deviations of the pollen concentrations varied significantly on the daily scale; a possible explanation for why no significant differences in previous work (Rojo et al., 2019). 

Fully understanding all source of errors related to traditional manual measurements is particularly important given the advent of automatic pollen monitors based on new technologies such as image-recognition (Oteros et al., 2019?) and air-flow cytometry (Sauvageat et al., 2019; Crouzy et al., 2016). For validation purposes, these new, real-time instruments are compared against traditional, manual observations and it is thus essential to understand what level of error can be expected from the various measurement techniques to make robust comparisons. 

In this study we compare measurements from three Hirst-type traps placed side-by-side in Payerne, Switzerland, from the pollen season of 2013. While these observations date back somewhat, counting methods have not changed significantly since and the data remain valid. We aim to investigate more fully the variability between measurements at the same location at both the daily and hourly scale. 

# Data

Three Hirst-type traps (Burkard Manufacturing, UK) were co-located on the roof of the Federal Office of Meteorology and Climatology MeteoSwiss building in Payerne, Switzerland. The roof is flat and at a height of 7m. The Hirst-type traps were placed in a line, approximately 3m from each other. The study period ran from 26 March to 25 July 2013 and the three Hirst-type traps operated simultaneously for this entire period.

The three Hirst-type traps (numbered 2, 4, and 8) were carefully calibrated and set for seven-day sampling using Melinex tape coated with silicon (Lanzoni, Italy). The volume of air sampled was checked each week using a flow meter when the drums were exchanged. Tapes were cut into seven daily segments and mounted onto microscope slides. Two longitudinal scans were counted on each slide using an Olympus microscope with a 600x magnification and, at a later stage, two additional lines were counted at 400x magnification. An average of all four lines was taken to calculate the final concentrations. P Data are expressed as daily average concentrations (pollen grains/m3). 
All pollen types were counted and included in the “total pollen” category. We also selected 20 individual pollen types for a more precise comparison: Urtica, Castanea, Salix, Platanus, Betula, Alnus, Corylus, Fraxinus, Rumex, Taxus, Cupressus, Poaceae, Ulmus, Populus, Plantago, Quercus, Fagus Juglans Carpinus, Picea, and Pinus. This list aims to include the most important pollen types measured during the 2013 campaign in Payerne and to cover wide range of pollen sizes to provide a better understanding of how pollen counts vary with grain size. 

## Import

```{r}

# The data was provided by Fiona Tummon, in form of 10-minute pollen counts per line and trap.
# The data prep until that point was not re-evaluated.
# The datetimes were provided in a seperate file (in Payerne local time)
# PAY2, PAY4 and PAY8 are three different pollen traps
# lines 2, 4, 6, 7 are four different lines, along which the pollen were manually counted under the microsope.
# Lines 2 and 6 were counted in 2013, whereas lines 4 and 7 were counted in 2019

path_data <- paste0(here::here(), "/ext-data/")
files_data <- list.files(path_data, pattern = "2013_10min.csv")
files_datetimes <- list.files(path_data, pattern = "dates_10min.csv")

data_raw <-
  map(files_data, ~ read_delim(
    paste0(path_data, .x),
    delim = ";",
    col_names = TRUE,
    col_types = cols(.default = "n"),
    na = "nan"
    )
  ) %>% 
  setNames(files_data)

dates_raw <-
  map(files_datetimes,~ read_delim(
    paste0(path_data, .x),
    delim = ";",
    col_names = FALSE,
    col_types = cols(.default = "n") # dates and times are in a weird format, have to import as numeric as a consequence
    )
  ) %>% 
  setNames(files_datetimes)

dates <- map(dates_raw, ~mutate(
  .x,
  time = sprintf("%04d", as.integer(X2)), # otherwise we lose leading zeros
  datetime = ymd_hm(paste0(X1, time)),
  datetime = datetime - hours(2), # Convert from CEST to UTC
  datetime_adj = datetime - minutes(10), # This is necessary to cope with the guidelines about hourly averages mentioned below
  hour = hour(datetime_adj) + 1,
  date = date(datetime_adj)) %>% 
  select(datetime, date, hour)
  )

# Combining the measurements with the datetimes for the three traps
# It appears that the traps have slightly different observations (e.g. 9 minutes past the full hour instead of 10). This will have no impact, once the concentrations are averaged per hour
data_raw <- map2(data_raw, rep(1:3, each = 4), ~bind_cols(.x, dates[[.y]]) %>% mutate(trap = 2^(.y)))

# Create a new variable to differentiate between counted lines; note that traps were already defined in the previous map-call above
data_raw <- map2(data_raw, rep(c(2, 4, 6, 7), 3), ~mutate(.x, line = .y))

# We are only interested on a subset of the data. The other species either have very low measurements in Switzerland, or they are not relevant from a medical point of view. This selection has been discussed with Regula Gehrig and has been used for a longer time now at MCH.
# The order of variables is different in the PAY2 and PAY6 data, hence we quicly sort them here:
species_selection <- c("Castanea", "Alnus", "Ulmus", "Cupressus", "Fraxinus", "Fagus", "Juglans", "Plantago", "Corylus", 
    "Pinus", "Quercus", "Rumex", "Platanus", "Populus", "Poaceae", "Salix", "Betula", "Carpinus", 
    "Urtica", "Taxus", "Picea")

data_raw <- map(data_raw, ~select(.x, 
    kacastz0, kaalnuz0, kaulmuz0, kacuprz0, kafraxz0, kafaguz0, kajuglz0, khplanz0, kacoryz0, 
    kapinuz0, kaquerz0, khrumez0, kaplatz0, kapopuz0, khpoacz0, kasaliz0, kabetuz0, kacarpz0, 
    khurtiz0, kataxuz0, kapicez0, datetime, date, hour, trap, line) %>% 
  setNames(c(species_selection, "datetime", "date", "hour", "trap", "line")))

```

## Data Bias Correction

The team in Payerne found that the air sucking rates of the Hirst traps are actually higher in reality than reported by the manufacturer. The traps suck in 13.5 l per minute instead of 10 l per minute. Hence our Pollen counts per 10 minutes are too high and should be reduced by a factor of 1.35.

```{r}
data_raw <- map(data_raw, ~mutate_at(.x, vars(all_of(species_selection)), ~./1.35))
```


## Imputation and Timeframe Selection

There are a few missing measurements.
Generally, if one species is NA in one specific trap and line at a certain time, then all species are NA.
Hence we could assume that the traps had a malfunction during that time.
What is surprising though is the fact that not all lines have the same amount of NAs.
This might suggest that some entries were omitted during the manual pollen counting exercise.
If these values are outside of the blooming season then we don't have an issue. This should be further evaluated once a decision was made on which species will be evaluated in the paper.


```{r}

# # Locate Missing Data (to present to Fiona)
# 
# missing <- data_raw %>% 
#   bind_rows %>% 
#   filter_all(any_vars(is.na(.))) %>% 
#   select(datetime, date, hour, trap, line) 
# 
# missing %>% 
#   write.csv(file = "missing.csv")
# 
# missing %>% 
#   group_by(trap, line, date) %>% 
#   summarise() %>% 
#   ungroup() %>% 
#   mutate(dummy = date) %>% 
#   pivot_wider(values_from = date, names_from = c(trap, line)) %>% 
#   write.csv("overview.csv")

# map2(data, names(data), ~Amelia::missmap(as.data.frame(.x), y.labels = "", y.at = 1, col = swatch()[c(4,2)], main = paste("Missingnes Map for", .y)))
data_raw[[1]] %>% as.data.frame() %>% Amelia::missmap(y.labels = "", y.at = 1, col = swatch()[c(4,2)], main = "Missingness Map for Trap 2 - Line 2")

```

For now we leave these measurements as NA and will not plot them nor impute values for statistical analyses.

```{r}
# Make sure that all dataframes have the same amount of rows, working with max amount of data available
# It's probably better to work with full days only, so that each measurement independet of the aggregation window has the same meaning
data_raw <- map(data_raw, ~.x %>% filter(datetime <= as_datetime("2013-06-30 00:00:00"),
                                 datetime >= as_datetime("2013-04-02 00:10:00"))) 
```


```{r}
# How often is it actually the case that only a part of the three traps measured pollen at any given time:
# First we need to make sure that all measurements are exactly registered at 10, 20, 30, 40, 50, 0 minutes after the hour.

data_raw <- map(data_raw, ~.x %>% 
  mutate(datetime = if_else(minute(datetime) %% 10 != 0, datetime + minutes(1), datetime)))
  
# minute(data_raw %>% bind_rows() %>% pull(datetime)) %>% unique() # It's always plus one to fix it if necessary at all.

```

We are mostly interested in observations where Pollen are actually being measured and not the ones where no trap detected anything (which is usually true for many days outside the blooming season). This will lead to a large increase in NA values (that will not be plotted or used for statistical comparison). This is of course a deliberate choice, it was not the goal of the study to see how often all traps measured zero pollen. Everything strongly depends on this decision and the data preparation should be adjusted if other statistical questions are being evaluated.
We might be switching up the data prep after the first meeting to make the data more consistent and the resulting residuals from the model fit easier to work with. So instead of setting value to NA when all traps are zero for any given time, we will set value to zero if at least ONE trap is zero at a given time. This has a potentially large effect on the analysis and should be reevaluated if the code is used for other problems again in the future. TBD!

```{r}

# data <- data_raw %>%
#   bind_rows %>%
#   group_by(datetime) %>%
#   mutate_at(vars(all_of(species_selection)), function(x){
#       if(sum(x, na.rm = TRUE) > 0)
#         return(x)
#       else
#         return(NA_real_)
#   }) %>%
#   ungroup %>%
#   group_split(trap,line)

measurements_to_exclude <- data_raw %>%
  bind_rows %>%
  group_by(trap, datetime) %>%
  replace(is.na(.), 0) %>%
  mutate_at(vars(all_of(species_selection)), .funs = list(exclude = ~mean(.))) %>%
  ungroup() %>%
  group_by(datetime) %>%
  mutate_at(vars(all_of(paste0(species_selection, "_exclude"))), function(x){
      if(all(as.integer(x) == 0))
        return(NA) # This will set all measurements to NA if one of the traps did not measure any pollen
      else
        return(0)
  }) %>%
  ungroup()


measurements_after_exclusion <- map2_df(data_raw %>% bind_rows %>% select(species_selection), measurements_to_exclude %>% select(contains("exclude")), ~.x + as.integer(.y))

data <- measurements_after_exclusion %>%
  bind_cols(measurements_to_exclude %>%
              select(datetime, date, hour, trap, line)) %>%
  group_split(trap,line)

data[[1]] %>%  as.data.frame() %>% Amelia::missmap(y.labels = "", y.at = 1, col = swatch()[c(4,2)], main = "Missingness Map for Trap 2 - Line 2 after data Preparation")

```

## Aggregation

### By Species

In The following five groups were defined based on the Pollen-Grain sizes.

- Group 1: Urtica, Castanea
- Group 2: Alnus, Betula, Corylus, Fraxinus, Platanus, Rumex, Salix
- Group 3: Plantago, Poaceae, Populus, Taxus, Cupressus, Ulmus
- Group 4: Carpinus, Fagus, Juglans, Quercus
- Group 5: Pinus, Picea

```{r}

Group1 <- c("Urtica", "Castanea")
Group2 <- c("Alnus", "Betula", "Corylus", "Fraxinus", "Platanus", "Rumex", "Salix")
Group3 <- c("Plantago", "Poaceae", "Populus", "Taxus", "Cupressus", "Ulmus")
Group4 <- c("Carpinus", "Fagus", "Juglans", "Quercus")
Group5 <- c("Pinus", "Picea")

data <- map(data, ~.x %>% 
  mutate(Group1 = if_else(!is.na(Urtica) | !is.na(Castanea), 
                              rowSums(.[names(.x) %in% Group1], na.rm = TRUE), 
                              NA_real_),
         Group2 = if_else(!is.na(Alnus) | !is.na(Betula) | !is.na(Corylus) | !is.na(Fraxinus) | !is.na(Platanus) | !is.na(Rumex) | !is.na(Salix), 
                              rowSums(.[names(.x) %in% Group2], na.rm = TRUE), 
                              NA_real_),
         Group3 = if_else(!is.na(Plantago) | !is.na(Poaceae) | !is.na(Populus) | !is.na(Taxus) | !is.na(Cupressus) | !is.na(Ulmus), 
                               rowSums(.[names(.x) %in% Group3], na.rm = TRUE), 
                               NA_real_),
         Group4 = if_else(!is.na(Carpinus) | !is.na(Fagus) | !is.na(Juglans) | !is.na(Quercus), 
                                rowSums(.[names(.x) %in% Group4], na.rm = TRUE), 
                                NA_real_),
         Group5 = if_else(!is.na(Pinus) | !is.na(Picea),
                                rowSums(.[names(.x) %in% Group5], na.rm = TRUE), 
                                NA_real_),
         Total = if_else(!is.na(Group1) | !is.na(Group2) | !is.na(Group3) | !is.na(Group4) | !is.na(Group5), 
                         rowSums(.[names(.x) %in% species_selection], na.rm = TRUE), 
                         NA_real_)))

```


Five concentration classes could also be defined for the analysis: 0-10 pollen grains/m3, 10-20 pollen grains/m3, 20-50 pollen grains/m3, 50-100 pollen grains/m3, 100-300 grains/m3, and >300 pollen grains/m3, with the aim of better understanding whether different pollen concentrations have an impact on sampling and measurement error. This will be done on a ad-hoc basis when needed further below. The classes will depend on the average window.

### Temporal

One of the questions is, how long the observation period should be for the Pollen traps to produce robust results on any specific day.
The original time span is 10 minutes, we want to look at various average concentration (e.g. hourly, 3 hours, 6 hours, daily).

There are some general guidelines from Regula Gehrig on how to average concentration that should be followed here.

Die Stundenwerte werden wie folgt aus den 10-Minutenwerten berechnet:
z.B. Wert für 6:00 Uhr UTC wird berechnet aus 05:10 – 6:00 UTC

Die 3-h-Werte, die du auch im DWH findest, werden so berechnet:
z.B. Wert um 09:00 UTC: 06:10-09:00

Die Tageswerte werden bei den Pollen glaub ich aus den Stundenwerten berechnet aus:
01:00 – 24:00 (d.h. 00:00 – im DWH gibt es keinen Wert 24:00) UTC 

```{r}

# Hour 1 represents the duration 00:10 - 01:00, and same for all hours up to 24
data_hourly <- map(data, ~.x %>% 
  group_by(date, hour, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup())


# # The data comes in a sorted format by date and time, below I check that the start and endpoint is the same when averaged hourly
# # This is actually the case with the first measurement on 2013-04-01 11 o'clock and the last on 2013-07-01 - 10 o'clock (as trimmed above).
# # All lines and traps show the same observations.
# map(data_hourly, ~.x %>%
#   slice(c(1, nrow(.x))) %>%
#   rowid_to_column) %>%
#   bind_rows() %>%
#   arrange(rowid)
# # All the traps have the same amount of unique observations
# map(data_hourly, ~.x %>% select(date, hour) %>% unique %>% nrow)
# map(data_hourly, ~.x %>% slice(tail(row_number(), 10)))

data_hours3 <- map(data, ~.x %>% 
  mutate(hours3 = case_when(
    hour >= 1 & hour < 3 ~ "06:00",
    hour >= 3 & hour < 6 ~ "12:00",
    hour >= 6 & hour < 9 ~ "18:00",
    hour >= 9 & hour < 12 ~ "24:00",
    hour >= 12 & hour < 15 ~ "06:00",
    hour >= 15 & hour < 18 ~ "12:00",
    hour >= 18 & hour < 21 ~ "18:00",
    hour >= 21 & hour < 24 ~ "24:00"
    )
  ) %>% 
  group_by(date, hours3, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup())

data_hours6 <- map(data, ~.x %>% 
  mutate(hours6 = case_when(
    hour >= 1 & hour <= 6 ~ "06:00",
    hour > 6 & hour <= 12 ~ "12:00",
    hour > 12 & hour <= 18 ~ "18:00",
    hour > 18 & hour <= 24 ~ "24:00"
    )
  ) %>% 
  group_by(date, hours6, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup())

data_hours12 <- map(data, ~.x %>% 
  mutate(hours12 = case_when(
    hour >= 1 & hour <= 12 ~ "12:00",
    hour > 12 & hour <= 24 ~ "24:00"
    )
  ) %>% 
  group_by(date, hours12, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup())

data_daily <- map(data_hourly, ~.x %>% 
                    group_by(date, trap, line) %>% 
                    summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
                    ungroup())


```

### Assess the Effect of Setting many Measurements to NA

```{r}
data_hourly_raw <- map(data_raw, ~.x %>%
  group_by(date, hour, trap, line) %>%
  summarise_at(vars(all_of(species_selection)), ~mean(., na.rm = TRUE)) %>%
  ungroup())

data_daily_raw <- map(data_hourly_raw, ~.x %>%
                    group_by(date, trap, line) %>%
                    summarise_at(vars(all_of(species_selection)), ~mean(., na.rm = TRUE)) %>%
                    ungroup())


number_of_measurements <- data_hourly %>% 
  bind_rows %>%   
  group_by(trap, date, hour) %>% 
  summarise_at(vars(all_of(species_selection)), ~mean(., na.rm = TRUE)) %>% 
  replace(is.na(.), 0) %>% 
  mutate_at(vars(all_of(species_selection)), funs(as.integer(as.integer(.) != 0))) %>% 
  ungroup %>% 
  group_by(date, hour) %>%
  summarise_at(vars(all_of(species_selection)), ~sum(., na.rm = TRUE)) %>% 
  ungroup 

bind_rows(number_of_measurements %>% 
    summarise_at(vars(all_of(species_selection)), ~sum(. == 3)),  
  number_of_measurements %>% 
    summarise_at(vars(all_of(species_selection)), ~sum(. == 2)),
  number_of_measurements %>% 
    summarise_at(vars(all_of(species_selection)), ~sum(. == 1)),
  number_of_measurements %>% 
  summarise_at(vars(all_of(species_selection)), ~sum(. == 0))) %>% 
  mutate(Comment = c("All traps measured", "Two traps measured", "One trap measured", "None measured")) %>% select(Comment, Fraxinus, Poaceae, Betula, Quercus, Pinus) %>% 
  kable() %>% 
  kable_styling()


```

# Tabular Comparison of Various Metrics

In the following when we look at measurements from traps, we simply average between the four counted lines.

Several metrics were calculated: the frequency of occurrence for each pollen was obtained by counting the number of days that each particular pollen was detected; the seasonal mean was calculated by averaging values for each pollen for all the days that it occurred; and the Seasonal Pollen Index (SPI) was calculated by integrating the concentrations of a particular pollen over the entire season (Mandrioli et al., 1998).

These metrics are well-known and usually part of a Pollen Measurement study. It is common to calculate the means before setting values outside the blooming season to NA. Hence I created a data_raw list of tibbles above just for that purpose.

```{r}
occurence_raw <- data_daily_raw %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection)), ~sum(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection)), ~sum(. !=0, na.rm = TRUE)) %>% 
  ungroup() 

names <- names(occurence_raw)

occurence_raw <- occurence_raw %>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  select(name, trap2, trap4, trap8) %>% 
  filter(name != "trap")

maximum_raw <- data_daily_raw %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection)), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection)), ~max(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

average_raw <- data_daily_raw %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection)), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection)), ~mean(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

spi_raw <- data_daily_raw %>% # Seasonal Pollen Integral
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection)), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection)), ~sum(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)


occurence_raw %>% 
  inner_join(maximum_raw, by = "name") %>% 
  inner_join(average_raw, by = "name") %>% 
  inner_join(spi_raw, by = "name") %>% 
  filter(name %in% species_selection) %>% 
  arrange(desc(trap8.y.y)) %>% 
  setNames(c("Name", rep(paste0("trap", c(2, 4, 8)), times = 4))) %>% 
  kable(escape = FALSE) %>%
  kable_styling(c("striped", "condensed"), full_width = FALSE) %>% 
  add_header_above(c(" " = 1, "Frequency of Occurence [#Days]" = 3, "Maximum Daily Concentration [#Pollen/m³]" = 3, "Average Daily Concentration [#Pollen/m³]" = 3, "Seasonal Pollen Integral [#Pollen/m³]" = 3)) %>% 
  column_spec(1, italic = TRUE) %>% 
  column_spec(2:13, width = "2.5cm") %>% 
  column_spec(seq(1, 13, 3), border_right = TRUE)

```

As described above we averaged the Pollen Counts from the 4 lines per trap and can now compare the traps based on Various Metrics.
Typically we see that trap 8 shows higher measurements for a variety of species. Once the species and temporal-average are fixed, this can be discussed in more detail.

# Visualization

Now let's plot some time series to get a better overview of the data. In the plot below the user can select any species and temporal resolution to compare the measurements from different lines and traps. After that we always average between the lines for each trap, to obtain a representative sample (~10% of pollen-plate evaluated under microscope). As a very general statement, we see again that trap number 8 has higher concentration-measurements than the other two. The shorter the temporal averaging window the large those differences can become. Generally, the analysis tends to be more stable when looking at larger time-windows and Total-Pollen counts. This is not surprising, as these setting allow us to benefit from all the data we have and missing measurements will be excluded during the averaging of concentrations in a time-window.

```{r }

plot <- plot_pollen(species = species, resolution = resolution, group = "trap", traps = c(2, 4, 8), rm_zeros = TRUE)
plot
# ggsave(filename = "plot.png", plot = plot, width = 8, height = 4.5)

```


# Statistical Comparison

The analysis should also serve as a base for a scientific publication. We want to assess the robustness and similarity of the Hirst traps. For that purpose we will compare the measured Pollen concentrations for the three different traps. We will assess the similarity between the three traps and test their robustness for different averaging windows. This task of comparing measures has been discussed in scientific literature for many decades and it is not an easy task. I will apply many different approaches to visually and mathematically check the similarity between the three measurements and will add some remarks about their pros, cons and underlying assumptions.. It is in the nature of the problem that no conclusion will be made at the end that holds with a 100% certainty. 

Another goal of this analysis is to set up a pipeline (i.e. code construct) that can be sued to compare newer automatic pollen traps to the traditional Hirst traps. For that reason I try to keep the code as flexible and well structured as possible.

These settings are crucial and should always be remembered when running the chunks below:

- The temporal resolution for the calculation of the mean concentrations
- The species or group of Pollen concentrations, to distinguish between high and low blooming season
- The threshold below which Pollen measurements are set to NA (because they become unreliable) is set to 0 currently

We will first visually evaluate the similarity using Altman-Bland and Correlation Plots and then carry out some statistical tests.
There are two pathways to evaluate the similarity of the measurements between the traps (measures). One can either transform the measurements (log), so that they are normally distributed and then work with ANOVA, F-Test and Tukey Honestly Significance Difference (HSD); or one can take the measurements as is (counts per m³) and work with the non-parametric Kruskal-Wallis Test that only requires rank-symmetry but doesn't assume any underlying distribution of the data. For the non-parametric contrasts multiple options present themselves that we will investigate below.

```{r}

if (resolution == "daily"){
  data_anova <- map(data_daily, ~.x %>%
                     mutate(timestamp = date))
} else if (resolution == "12hour"){
  data_anova <- map(data_hours12, ~.x %>%
                     mutate(timestamp = ymd_hm(paste0(date, hours12))))
} else if (resolution == "6hour"){
  data_anova <- map(data_hours6, ~.x %>%
                     mutate(timestamp = ymd_hm(paste0(date, hours6))))
} else if (resolution == "3hour"){
  data_anova <- map(data_hours3, ~.x %>%
                     mutate(timestamp = ymd_hm(paste0(date, hours3))))
} else if (resolution == "hourly"){
  data_anova <- map(data_hourly, ~.x %>%
                     mutate(timestamp = ymd_hm(paste0(date, paste0(sprintf("%02d", hour), ":00")))))
}

data_anova <- data_anova %>% 
  bind_rows() %>% 
  mutate(trap = as.factor(trap)) %>% 
  group_by(trap, timestamp) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup 

data_transformed <- data_anova %>% 
  mutate_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~log(. + 1))
  
# There are various methods to deal with zeros during log transformation
# 
#     Add a constant value © to each value of variable then take a log transformation - i will add x + 1
#     Impute zero value with mean. - not evaluated
#     Take square root instead of log for transformation - does not really help much

```


## Correlation

The correlation between the traps can be calculated easily and then the CI and p-values must be adjusted for multiple comparison.
The corr-test function from the psych handily offers this functionality.

Careful the correlation coefficients method have some serious shortcomings:

The correlation coefficient measures linear agreement--whether the measurements go up-and-down together. Certainly, we want the measures to go up-and-down together, but the correlation coefficient itself is deficient in at least three ways as a measure of agreement. (http://www.jerrydallal.com/LHSP/compare.htm)

- The correlation coefficient can be close to 1 (or equal to 1!) even when there is considerable bias between the two methods. For example, if one method gives measurements that are always 10 units higher than the other method, the correlation will be 1 exactly, but the measurements will always be 10 units apart.
- The magnitude of the correlation coefficient is affected by the range of subjects/units studied. The correlation coefficient can be made smaller by measuring samples that are similar to each other and larger by measuring samples that are very different from each other. The magnitude of the correlation says nothing about the magnitude of the differences between the paired measurements which, when you get right down to it, is all that really matters.
- The usual significance test involving a correlation coefficient-- whether the population value is 0--is irrelevant to the comparability problem. What is important is not merely that the correlation coefficient be different from 0. Rather, it should be close to (ideally, equal to) 1! 

### Pearson, Spearman and Kendall Correlation

A good summary of the methods and their shortcomings can be found here: https://www.statisticssolutions.com/correlation-Pearson-Kendall-spear man/

Generally the traps show a high level of correlation / association (well above 0.5). It looks like small concentrations lead to the largest discrepancies between the traps.

```{r}

methods <- c("pearson", "spearman", "kendall")

data_corr <- data_transformed %>% # For the robust methods (spearman, kendall) it doesn't matter whether the transformed data is used or the original
  select(!!sym(species), trap, timestamp) %>% 
  pivot_wider(names_from = trap, values_from = !!sym(species), timestamp) %>% 
  setNames(c("timestamp", paste0("trap", c(2, 4, 8))))

corr_matrix <- map(methods, ~corr.test(
  data_corr %>% select(-timestamp),
  use = "complete",
  method = .x,
  adjust = "holm",
  alpha = .05,
  ci = TRUE,
  minlength = 5
  ))

# dummy <- map(corr_matrix, ~.x %>% print(short = FALSE))


```

```{r}

ci <- map(corr_matrix, ~.x %>% 
  pluck(10)) %>% 
  bind_rows() %>% 
  round(2) %>% 
  mutate(method = rep(methods, each = 3)) %>%  
  mutate(ci = paste0("R-", tools::toTitleCase(method), ": ", lower.adj, " - ", upper.adj)) %>% 
  pull(ci)

gg1 <- data_corr %>% 
  ggplot(aes(x = trap2, y = trap4)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(alpha = 0.1) +
  geom_abline(slope = 1, intercept = 0, col = swatch()[4]) +
  geom_label(label = c(paste(ci[1], "\n", ci[4], "\n", ci[7])), aes(x = 3.6, y = 9)) +
  coord_cartesian(ylim = c(0, 10), xlim = c(0, 10))

gg2 <- data_corr %>% 
  ggplot(aes(x = trap2, y = trap8)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(alpha = 0.1) +
  geom_abline(slope = 1, intercept = 0, col = swatch()[4]) +
  geom_label(label = c(paste(ci[2], "\n", ci[5], "\n", ci[8])), aes(x = 3.6, y = 9)) +
  coord_cartesian(ylim = c(0, 10), xlim = c(0, 10))

gg3 <- data_corr %>% 
  ggplot(aes(x = trap4, y = trap8)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(alpha = 0.1) +
  geom_abline(slope = 1, intercept = 0, col = swatch()[4]) +
  geom_label(label = c(paste(ci[3], "\n", ci[6], "\n", ci[9])), aes(x = 3.6, y = 9)) +
  coord_cartesian(ylim = c(0, 10), xlim = c(0, 10))

title <- tools::toTitleCase(paste0("Comparison of ", resolution, " average concentrations of ", species, " pollen between the three traps"))

ggarrange(gg1, gg2, gg3, ncol = 3) %>%
  annotate_figure( top = title, bottom = text_grob("Pairwise correlation between traps; grey line shows the Loess smother; the red line shows a theroratical perfect correlation of 1. \n In the text box one can see the 95% confidence intervals of the R-values (adjusted for multiple comparison) as obtained by Pearson and two robust methods.", color = swatch()[1], face = "italic", size = 10))
```



### Altman-Bland Plots

The well established AB-method for clinical trials can be used here as well to compare the means and differences between traps. Again we see that trap 8 generally has higher measurements and that low concentrations lead to largest differences between the traps. The points lie within the two SD-line for the differences and hence the traps can be assumed to be strongly associated with each other. We also observe larger scattering of the points for lower concentrations.

```{r fig.height=9}

data_altman <- data_transformed %>% 
  select(!!sym(species), trap, timestamp) %>% 
    pivot_wider(names_from = trap, values_from = !!sym(species), timestamp) %>% 
    setNames(c("timestamp", paste0("trap", c(2, 4, 8)))) %>% 
    mutate(mean24 = if_else(!is.na(trap2) | !is.na(trap4),
                          rowSums(.[2:3], na.rm = TRUE) / 2, 
                          NA_real_),
           mean28 = if_else(!is.na(trap2) | !is.na(trap8),
                          rowSums(.[c(2, 4)], na.rm = TRUE) / 2, 
                          NA_real_),
           mean48 = if_else(!is.na(trap4) | !is.na(trap8),
                          rowSums(.[3:4], na.rm = TRUE) / 2, 
                          NA_real_),
           diff24 = trap4 - trap2,
           diff28 = trap8 - trap2,
           diff48 = trap8 - trap4)

sd_diff <- data_altman %>% 
  select(starts_with("diff")) %>% 
  summarise_all(~sd(., na.rm = TRUE)) %>% 
  pivot_longer(1:3, values_to = "sd", names_to = "dummy") %>% 
  pull(sd)

gg_ab1 <- data_altman %>% 
  ggplot(aes(x = mean24, y = diff24)) +
  geom_point(alpha = 0.5) +
  coord_cartesian(xlim = c(3, 8), ylim = c(-sd_diff[1] * 3, sd_diff[1] * 3)) +  
  geom_abline(slope = 0, intercept = 0, col = swatch()[4], alpha = 0.8) +
  geom_abline(slope = 0, intercept = sd_diff[1] * 2, col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_abline(slope = 0, intercept = sd_diff[1] * (-2), col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_smooth(alpha = 0.1) +
  labs(y = "Difference(trap4 - trap2)", x = "Mean(trap4, trap2)")
  
gg_ab2 <- data_altman %>% 
  ggplot(aes(x = mean28, y = diff28)) +
  geom_point(alpha = 0.5) +
  coord_cartesian(xlim = c(3, 8), ylim = c(-sd_diff[2] * 3, sd_diff[2] * 3)) +  
  geom_abline(slope = 0, intercept = 0, col = swatch()[4], alpha = 0.8) +
  geom_abline(slope = 0, intercept = sd_diff[2] * 2, col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_abline(slope = 0, intercept = sd_diff[2] * (-2), col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_smooth(alpha = 0.1) +
  labs(y = "Difference(trap8 - trap2)", x = "Mean(trap8, trap2)")

gg_ab3 <- data_altman %>% 
  ggplot(aes(x = mean48, y = diff48)) +
  geom_point(alpha = 0.5) +
  coord_cartesian(xlim = c(3, 8), ylim = c(-sd_diff[3] * 3, sd_diff[3] * 3)) +  
  geom_abline(slope = 0, intercept = 0, col = swatch()[4], alpha = 0.8) +
  geom_abline(slope = 0, intercept = sd_diff[3] * 2, col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_abline(slope = 0, intercept = sd_diff[3] * (-2), col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_smooth(alpha = 0.1) +
  labs(y = "Difference(trap8 - trap4)", x = "Mean(trap8, trap4)")

title <- "Altman-Bland Plots for all Traps"

ggarrange(gg_ab1, gg_ab2, gg_ab3, nrow = 3) %>%
  annotate_figure( top = title, bottom = text_grob("Pairwise comparison between traps; grey line shows the Loess smother; \n the red line shows a theroratical perfect agreement between two traps of zero. \n The dashed red line shows the 2 * sd of the differences, where we expect the points to lie within.", color = swatch()[1], face = "italic", size = 12))
  

```

## Parametric / ANOVA

First we compare the traps with the traditional ANOVA approach. Statistical inference (p-values, confidence intervals, . . . ) is only valid if the model assumptions are fulfilled. So far, this means (many paragraphs are quoted from Lukas Meier ETH - Script Applied Statistics ANOVA Course):

- are the errors independent?
- are the errors normally distributed?
- is the error variance constant?
- do the errors have mean zero?

The first assumption is most crucial (but also most difficult to check). If the independence assumption is
violated, statistical inference can be very inaccurate. In the ANOVA setting, the last assumption is typically
not as important compared to a regression setting, as we are typically fitting “large” models.

Below we prepare the data averaging between the four lines in each trap. As we will see further below in the residuals section, it is necessary to logarithmic the measured concentrations.

### F-Test / Omnibus-Test

We first test for an overall significance of the trap on the measurements.
F = variation between sample means / variation within the samples. I If the P-Value is larger than 5% we don't have to assume that the Null-Hypothesis (measurements from all traps originate from the same distribution).
We use the settings that the coefficients for the three traps add to zero.

```{r}
fit_anova <- aov(as.formula(paste(species, "~ trap")), data = data_transformed, contrasts = c("contr.sum", "contr.poly"))
summary(fit_anova)

```

### Residual Analysis

As mentioned above (and already applied), the residuals of the anova fit need to fulfill some assumptions that we want to check in the following.

- are the errors normally distributed?

In a QQ-plot we plot the empirical quantiles (“what we see in the data”) vs. the theoretical quantiles (“what
we expect from the model”). The plot should show a more or less straight line if the distributional assumption
is correct. By default, a standard normal distribution is the theoretical “reference distribution”.

They are definitely not and we have to do some adjustments. So for the following plot we logarithmic the data to deal with the right-skewedness. The best results were achieved by first logarithmic the data and then taking the square root.

```{r}
fit_anova_raw <- aov(as.formula(paste(species, "~ trap")), data = data_anova, contrasts = c("contr.sum", "contr.poly"))

gg_res1 <- tibble(residuals = residuals(fit_anova_raw, type = "pearson")) %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line()

gg_res2 <- tibble(residuals = residuals(fit_anova, type = "pearson")) %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line()

ggarrange(gg_res1, gg_res2, nrow = 1) %>% 
  annotate_figure(top = "QQ-Plot for the ANOVA Residuals With (left) and Without Logarithmizing")

```


- is the error variance constant?
- do the errors have mean zero?

The Tukey-Anscombe plot plots the residuals vs. the fitted values. It allows us to check whether the residuals have constant variance and whether the residuals have mean zero (i.e. they don’t show any deterministic pattern). We don't plot the smoothing line as loess (and other) algorithms have issues when the same value is repeated a large number of times (jitter did not really help).

```{r}
# plot(fit_anova, which = 1)

gg_tukey1 <- tibble(resid = residuals(fit_anova_raw, type = "pearson"), fitted = fit_anova_raw$fitted.values) %>%
  ggplot(aes(x = fitted, y = resid)) +
  geom_point(alpha = 0.5, position = position_jitter(width = 5, height = 0)) +
  # geom_smooth(method = "loess", col = swatch()[4]) + # We need to add some jitter, because the same concentrations are repeated a larger number of time: https://stackoverflow.com/questions/38864458/loess-warnings-errors-related-to-span-in-r but it does not really help too much, i will use the base plot for now.
  geom_abline(slope = 0, intercept = 0, col = swatch()[3]) +
  coord_cartesian(ylim = c(-500, 500))

gg_tukey2 <- 
  tibble(resid = residuals(fit_anova, type = "pearson"), fitted = fit_anova$fitted.values) %>%
    ggplot(aes(x = fitted, y = resid)) +
    geom_point(alpha = 0.5, position = position_jitter(width = 0.02, height = 0)) +
    # geom_smooth(method = "loess", col = swatch()[4]) + # We need to add some jitter, because the same concentrations are repeated a larger number of time: https://stackoverflow.com/questions/38864458/loess-warnings-errors-related-to-span-in-r but it does not really help too much, i will use the base plot for now.
    geom_abline(slope = 0, intercept = 0, col = swatch()[3])+
  coord_cartesian(ylim = c(-3, 3))

ggarrange(gg_tukey1, gg_tukey2)  %>% 
  annotate_figure(top = "Tukey Anscombe - Plot for the ANOVA Residuals With (left) and Without Logarithmizing")

```


- are the errors independent?

If the data has some serial structure (i.e., if observations were recorded in a certain time order), we typically
want to check whether residuals close in time are more similar than residuals far apart, as this would be a
violation of the independence assumption. We can do so by using a so-called index plot where we plot the
residuals against time. For positively dependent residuals we would see time periods where most residuals
have the same sign, while for negatively dependent residuals, the residuals would “jump” too often from
positive to negative compared to independent residuals.

```{r}

resid <- residuals(fit_anova_raw, type = "pearson")
resid_df <- tibble(resid = resid, id = as.numeric(names(resid)))

gg_timeline1 <- tibble(id = 1:nrow(data_anova), time = data_anova$timestamp) %>%
  left_join(resid_df, by = "id") %>% 
  ggplot(aes(x = time, y = resid)) +
  geom_point() +
  geom_line(alpha = 0.3)

resid <- residuals(fit_anova, type = "pearson")
resid_df <- tibble(resid = resid, id = as.numeric(names(resid)))

gg_timeline2 <- tibble(id = 1:nrow(data_anova), time = data_anova$timestamp) %>%
  left_join(resid_df, by = "id") %>% 
  ggplot(aes(x = time, y = resid)) +
  geom_point() +
  geom_line(alpha = 0.3)

ggarrange(gg_timeline1, gg_timeline2, nrow = 2) %>% 
  annotate_figure(top = "Index-Plot for the ANOVA Residuals With (left) and Without Logarithmizing")

```

### Contrasts / Tukey HSD

The F-test is rather unspecific. It basically gives us a “Yes/No” answer for the question “is there any
treatment effect at all?”. It doesn’t tell us what specific treatment (or treatment combination) is significant.
Quite often we have a more specific question than the aforementioned global null hypothesis. E.g., we might
want to compare a set of new treatments vs. a control treatment or we want to do pairwise comparisons
between many (or all) treatments.
Multiple Testing: The problem with all statistical tests is the fact that the (overall) error rate increases with increasing number
of tests.

Tukey's test compares the means of every treatment to the means of every other treatment; that is, it applies simultaneously to the set of all pairwise comparisons and identifies any difference between two means that is greater than the expected standard error. In other words, the Tukey method is conservative when there are unequal sample sizes. 
Assumptions:
- The observations being tested are independent within and among the groups.
- The groups associated with each mean in the test are normally distributed.
- There is equal within-group variance across the groups associated with each mean in the test (homogeneity of variance).

https://en.Wikipedia.org/wiki/Tukey%27s_range_test

```{r}
hsd <- TukeyHSD(fit_anova)
plot(hsd)
```

### Random Effects Model

So far we have been looking at three traps in isolation. But they are part of a larger "population" of Hirst traps being used at MCH or worldwide. Hence we also should look at ANOVA with trap as a random effect. This needs a bit more data per trap, otherwise the model fitting does not reach convergence and produce a so-called singular fit. Parameter estimation for the variance components is typically being done with a technique called restricted maximum likelihood (REML). We could also use “classical” maximum-likelihood estimators here, but REML estimates are less biased. The parameter is estimated with maximum-likelihood assuming that the variances are known.
We want to compare the random effect variance for trap with the Residual variance here.

```{r}
fit_lmer <- lmer(as.formula(paste(species, "~ (1 | trap)")), data = data_transformed)
# isSingular(fit_lmer) # This has to be checked
summary(fit_lmer)
confint(fit_lmer, oldNames = FALSE)
# ranef(fit_lmer)
# confint(fit_anova) # For comparison
```

So we can compare the estimated "explained variance" by trap of the total variance for both anova and lmer.

```{r}
summary_anova <- summary(fit_anova)
summary_lmer <- summary(fit_lmer)

explained_anova <- summary_anova[[1]][1, 2] / summary_anova[[1]][2, 2]
p_anova <- summary_anova[[1]][1, 5]

explained_lmer <- summary_lmer$varcor$trap[1, 1] / summary_lmer$sigma^2

tibble("ANOVA" = explained_anova,
       "LMER" = explained_lmer) %>% 
  kable() %>%
  kable_styling(full_width = FALSE)

```



Checking the residuals again here:

```{r}
par(mfrow = c(1, 2))
qqnorm(ranef(fit_lmer)$trap[, "(Intercept)"], main = "Random effects")
qqnorm(resid(fit_lmer), main = "Residuals")
plot(fit_lmer)
```

## Non-Parametric / Rank-Based Symmetrical

### Kruskal-Wallis Test / Omnibus Test

Kruskal-Wallis test by rank is a non-parametric alternative to one-way ANOVA test, which extends the two-samples Wilcoxon test in the situation where there are more than two groups. It’s recommended when the assumptions of one-way ANOVA test are not met. This tutorial describes how to compute Kruskal-Wallis test in R software. (http://www.sthda.com/english/wiki/kruskal-wallis-test-in-r)

In this case the assumptions are sometimes not met for specific species specially when looking at shorter time windows.

Assumptions

The assumptions of the Kruskal-Wallis test are similar to those for the Wilcoxon-Mann-Whitney test.

- Samples are random samples, or allocation to treatment group is random. 
- The two samples are mutually independent. 
- The measurement scale is at least ordinal, and the variable is continuous. 
- If the test is used as a test of dominance, it has no distributional assumptions. If it used to compare medians, the distributions must be similar apart from their locations. 

The test is generally considered to be robust to ties. However, if ties are present they should not be concentrated together in one part of the distribution (they should have either a normal or uniform distribution)
https://influentialpoints.com/Training/kruskal-wallis_anova-principles-properties-assumptions.htm

The Wilcoxon signed-rank test assumes that the data are distributed symmetrically around the median. In other words, there should be roughly the same number of values above and below the median. This can be checked by visual inspection using histogram or density estimators

```{r}
data_transformed %>% 
  ggplot(aes(x = !!sym(species))) +
  geom_histogram(alpha = 0.7, bins = 50) +
  # geom_freqpoly(col = swatch()[3], bins = 30) +
  # geom_density(col = swatch()[5]) +
  geom_vline(xintercept = median(data_transformed %>% pull(!!sym(species)), na.rm = TRUE), col = swatch()[4]) +
  ggtitle("Histogram of the Transformed Data With the Median (Red Line)")

```


```{r}
kruskal.test(as.formula(paste(species, "~ trap")), data = data_transformed)
```

### Dunn's test and the Conover-Iman test

The Dunn and Conover Test are typically used as a follow up to the Omnibus test above. It is appropriate to use those test and not the pairwise Wilcoxon Tests for the following two reason: 

- The rank sum test uses different ranks than those employed in the Kruskal-Wallis test (i.e. in both tests you mush the observations together, then rank them, then separate the ranks by group—the rank sum ignores the ranks you got with the omnibus test).
- The rank sum test does not use the pooled variance implied by the null hypothesis in the Kruskal-Wallis test (e.g., just as in one-way ANOVA where the post hoc t tests use an estimate of the pooled variance).

(https://stats.stackexchange.com/questions/71996/differences-between-dwass-steel-critchlow-fligner-and-mann-whitney-u-test-for-a)

The output following the Kruskal-Wallis test provides all possible pairwise comparisons (six in the case of four groups). So the one on the first row compares group B with group A, the first on the second row compares group C with group A, etc.).

The upper number for each comparison is Dunn's pairwise z test statistic. The lower number is in this example the raw p-value associated with the test.
, although this p-value changes depending on the family-wise error rate or false discovery rate multiple comparisons adjustment option. For step wise multiple comparison adjustments (e.g. Holm, Benjamini-Hochberg, etc.), the adjusted p-values will have an asterisk next to them if your would reject the null hypotheses at the specified significance level (which is not necessarily directly indicated by the adjusted p-values since rejection depends on ordering... see the documentation and citations therein for more details.).
https://stats.stackexchange.com/questions/126686/how-to-read-the-results-of-dunns-test

The null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half; this null hypothesis corresponds
to that of the Wilcoxon-Mann-Whitney rank-sum test. Like the ranksum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, Dunn's test may be understood as a test for median difference. 'dunn.test' accounts for tied ranks.

```{r}

dunn.test::dunn.test(data_transformed %>% pull(!!sym(species)),
                     data_transformed$trap,
                     method = "holm") # Try different FEWR /FDR adjustments here

conover.test::conover.test(data_transformed %>% pull(!!sym(species)),
                     data_transformed$trap,
                     method = "holm")

```

For Donover the ECDF curves are not allow to cross each other. This is actually not given for quite a few species and averaging windows. Maybe it's safer to use Dunn's test (they were usually comparable in terms of the return P-value statistics).

```{r}
data_transformed %>% 
  select(!!sym(species), trap) %>% 
  ggplot(aes(x = !!sym(species), col = trap)) +
  stat_ecdf(geom = "line", pad = FALSE) +
  ggtitle("Empirical Cumulative Distribution Function for All Traps")

```


```{r}
# # This is incorrect, see: https://stats.stackexchange.com/questions/71996/differences-between-dwass-steel-critchlow-fligner-and-mann-whitney-u-test-for-a
# 
# pairwise.wilcox.test(data_anova %>% pull(!!sym(species)), 
#                      data_anova$trap,
#                      paired = FALSE,
#                      p.adjust.method = "fdr",
#                      conf.int = TRUE)
```

### Random Effects Model

When using classic estimation methods, even one such outlier might inflate the between-group variability estimate and distort the results (see example discussed in Section 4). In such a case it would be natural to assume that the group’s random effect (or mean) is an outlier rather than all observations are outliers in the same direction. This concept of allowing potential contamination on different sources of variability leads to the “random effects contamination model”. With this model, we make the assumption of long-tailed or “gross error” distributions for the random effects as well and not just for the random errors. The effect of the contamination is then propagated via the design matrices to the actual observations. Levels of random variability can be hierarchical or crossed, or both, depending on the grouping structure in the data. This implies that the effect of a single outlier on the random effects level is not always as straight forward as in the above mentioned one-way anova example. The effect may be different for each observation as the result of an outlier for a single observation is combined with all the other random effects that affect this observation. This complex relationship between the source of contamination and what is effectively realized in the data can make it very hard or even impossible to spot contamination. This is where robust methods step in and help clear the picture. Basing the robust estimator on the “random effects contamination model” allows not only multiple sources of contamination, it also avoids unnecessary assumptions about the data’s grouping structure. The only assumption on the grouping structure, that is also required for classic estimation, is that the model parameters are estimable. Other contamination models usually assume that contamination is introduced and dealt with at the lowest level only – the level of the observations. In mixed-effects models, observations generally correlate with one another, and robust methods must respect these correlations. These dependencies between observations require other contamination models to make strict assumptions about the grouping structure. The random effects contamination model assumes that contamination occurs directly at the source of random variability, before the grouping structure is introduced, thus circumventing the complexity introduced by the data structure and avoids unnecessary assumptions.

Currently, there are two different methods available for fitting models. They only differ in how the consistency factors for the Design Adaptive Scale estimates are computed. Available fitting methods for theta and sigma. DAStau (default): For this method, the consistency factors are computed using numerical quadrature. This is slower but yields more accurate results. This is the direct analogue to the DAS-estimate in robust linear regression.

```{r}
fit_rlmer <- rlmer(as.formula(paste(species, "~ (1 | trap)")), data = data_transformed)
summary(fit_rlmer)

# # The parameters can be tuned for a more efficient fit, I leave this to the interested reader: 
# fit_rlmer2 <- update(fit_rlmer, rho.sigma.e = psi2propII(smoothPsi, k = 1.5), rho.sigma.b = psi2propII(smoothPsi, k = 1.5))
# compare(fit_lmer, fit_rlmer, fit_rlmer2, show.rho.functions = FALSE)
# gg <- map(1:3, ~plot(fit_rlmer, which = .x))

```

# Measurement Spread and Error Distribution

A second chapter of the paper should further elaborate on the relative errors between the traps.
We are mostly interested to observe the difference of one specific trap compared to the mean of the three traps at any given point in time. We will compare the relative error for different buckets of pollen concentrations, to see how errors behave when low amounts of pollen are measured. Furthermore, we would like to understand the shape of the error function and if possible fit a parametric curve to the relative differences from the mean.

In this first section we compare relative and absolute differences from the mean. Fitting parameters of a T-Distribution (distribution family visually chosen) using MLE (fitdistr / fit.st). The analysis is based on the transformed values, but the findings are usually independent of the log-transformation.

```{r warning=FALSE}
errors <- data_corr %>% 
  mutate(mean = if_else(!is.na(trap2) | !is.na(trap4) | !is.na(trap8),
                          rowSums(.[2:4], na.rm = TRUE) / 3, 
                          NA_real_)) %>% 
  mutate_at(vars(paste0("trap", c(2, 4, 8))), ~(. - mean)) %>% 
  pivot_longer(cols = trap2:trap8, values_to = "error", names_to = "trap") %>% 
  mutate(error_rel = error / mean) %>% 
  select(-mean)

sd(errors$error, na.rm = TRUE)
sd(errors$error_rel, na.rm = TRUE)

x <- seq(-2.5, 2.5, length.out = 10000)
t_shape <- fitdistr(errors$error[!is.na(errors$error)], "t") # QRM::fit.st(!is.na(errors$error)) #leads to same result without warning

m <- t_shape$estimate[1]
s <- t_shape$estimate[2]
df <- t_shape$estimate[3]
t_errors <- tibble(x = x, y = dt((x - m) / s, df = df) / s)

t_shape_rel <- fitdistr(errors$error_rel[!is.na(errors$error_rel)], "t") # QRM::fit.st(!is.na(errors$error)) #leads to same result without warning
m_rel <- t_shape_rel$estimate[1]
s_rel <- t_shape_rel$estimate[2]
df_rel <- t_shape_rel$estimate[3]
t_errors_rel <- tibble(x = x, y = dt((x - m_rel) / s_rel, df = df_rel) / s_rel)

gg_t_abs <- errors %>%
  ggplot(aes(x=error, y = ..density..)) +
  geom_histogram(bins= 100) +
  geom_density(col = swatch()[4]) +
  geom_rug(aes(y = 0), position = position_jitter(height = 0), col = swatch()[5]) +
  geom_line(data = t_errors, aes(x = x, y = y), col = swatch()[6]) +
  coord_cartesian(xlim = c(-5, 5))

gg_t_rel <- errors %>%
  ggplot(aes(x=error_rel, y = ..density..)) +
  geom_histogram(bins= 100) +
  geom_density(col = swatch()[4]) +
  geom_rug(aes(y = 0), position = position_jitter(height = 0), col = swatch()[5]) +
  geom_line(data = t_errors_rel, aes(x = x, y = y), col = swatch()[6]) +
  coord_cartesian(xlim = c(-2, 2))

ggarrange(gg_t_abs, gg_t_rel, nrow = 2) %>% 
  annotate_figure(top = "Absolute/Relative Differences from the Average Measurements of the Three Traps",
                  bottom = text_grob("On top the absolute differences from the mean fitted with a density kernel estimator (red) and a Student-t distribution (black). \n At the bottom the relative differences from the mean.", color = swatch()[1], face = "italic", size = 10))

```

```{r}

sample <- tibble(x = (x-m)/s, y = pt((x-m)/s, df = df))
sample_rel <- tibble(x = (x-m_rel)/s_rel, y = pt((x-m_rel)/s_rel, df = df))

gg_cdf1 <- errors %>% 
  ggplot(aes(x = error)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  coord_cartesian(xlim = c(-2.5, 2.5)) +
  geom_line(data = sample, aes(x = x, y = y))

gg_cdf2 <- errors %>% 
  ggplot(aes(x = error_rel)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  coord_cartesian(xlim = c(-0.5, 0.5)) +
  geom_line(data = sample_rel, aes(x = x, y = y))

ggarrange(gg_cdf1, gg_cdf2, nrow = 1) %>%
  annotate_figure(top = "Comparison of Empirical and Fitted CDF for Absolute (Left) and Relative Differences")
```
  
Now we can investigate the goodness of the fit of the selected t-Distribution above.
Graphically the QQ-Plot versus a t-distribution allows us to check whether the choice of distribution-family was sensible.

```{r}
errors %>% 
  mutate(error_scaled = (error)) %>%
  ggplot(aes(sample = error_scaled)) +
  stat_qq(distribution = qt, dparams = as.list(df)) +
  stat_qq_line(distribution = qt, dparams = as.list(df)) +
  ggtitle("QQ-Plot to Compare Empirical Quantiles with Theoretical Ones from a t-Distribution")
```

Then we can carry out two statistical tests that measure the goodness-of-fit.
Parameters have been estimated from data, so the tests in the following need to account for that (e.g. no KS-tests).

The Anderson–Darling and Cramér–von Mises statistics belong to the class of quadratic EDF statistics (tests based on the empirical distribution function. The Anderson–Darling test assesses whether a sample comes from a specified distribution. It makes use of the fact that, when given a hypothesized underlying distribution and assuming the data does arise from this distribution, the cumulative distribution function (CDF) of the data can be assumed to follow a uniform distribution. The data can be then tested for uniformity with a distance test (Shapiro 1980). [Wikipedia]

```{r}
cvm.test((errors$error[!is.na(errors$error)] - m)/s, "pt", df = df, estimated = TRUE)
cvm.test((errors$error_rel[!is.na(errors$error_rel)] - m_rel)/s_rel, "pt", df = df_rel, estimated = TRUE)

ad.test((errors$error[!is.na(errors$error)] - m)/s, "pt", df = df, estimated = TRUE)
ad.test((errors$error_rel[!is.na(errors$error_rel)] - m_rel)/s_rel, "pt", df = df_rel, estimated = TRUE)

```

## For Different Pollen Pollen Size Groups

```{r fig.height=9, eval = FALSE}
# set.seed(123)
gg <- list()
cvm <- numeric()
ad <- numeric()

for (j in c("Group1", "Group2", "Group3", "Group4", "Group5", "Total")){
  errors <- data_transformed %>% 
    select(!!sym(j), trap, timestamp) %>% 
    pivot_wider(names_from = trap, values_from = !!sym(j), timestamp) %>% 
    setNames(c("timestamp", paste0("trap", c(2, 4, 8)))) %>% 
    mutate(mean = if_else(!is.na(trap2) | !is.na(trap4) | !is.na(trap8),
                          rowSums(.[2:4], na.rm = TRUE) / 3, 
                          NA_real_)) %>% 
    mutate_at(vars(paste0("trap", c(2, 4, 8))), ~(. - mean)) %>% 
    pivot_longer(cols = trap2:trap8, values_to = "error", names_to = "trap") %>% 
    select(-mean)
  
  sd <- sd(errors$error, na.rm = TRUE)
  e <- simpleError("test error")
  t_shape <- try(QRM::fit.st(errors$error[!is.na(errors$error)])) # Actually lead to the same result as the bootstrapped version below
  # t_shape <- QRM::fit.mst(errors$error, method = "Brent", upper = 2, lower = 1, nit = 2000, tol = 1e-10) # https://magesblog.com/post/2013-03-12-how-to-use-optim-in-r/ or here https://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html
  class(t_shape)
  if (class(t_shape) != "try-error"){
    m <- t_shape$par.ests[2]
    s <- t_shape$par.ests[3]
    df <- t_shape$par.ests[1]
    t_errors <- tibble(x = x, y = dt((x - m) / s, df = df) / s)
    
    # If the parameteres are estimated from the data then this test applies the method from Braun and splits the data into two equally sized subsets. Therefore the test results are not stable, especially for smaller datasets (coarse temporal resolution). I will try to bootstrap the testing and the use the  mean (not 100% if this is valid, tbd).
    
    for (i in 1:1000) {
      cvm_test <- cvm.test((errors$error[!is.na(errors$error)] - m)/s, "pt", df = df, estimated = TRUE)
      ad_test <- ad.test((errors$error[!is.na(errors$error)] - m)/s, "pt", df = df, estimated = TRUE)
      
      cvm[i] <- cvm_test$p.value
      ad[i] <- ad_test$p.value
    }
    
    cvm <- mean(cvm)
    ad <- mean(ad)
  }
  
  gg[[j]] <- errors %>%
    ggplot(aes(x=error, y = ..density..)) +
    geom_histogram(bins= 100) +
    geom_density(col = swatch()[4]) +
    geom_rug(aes(y = 0), position = position_jitter(height = 0), col = swatch()[5]) +
    coord_cartesian(xlim = c(-2.5, 2.5)) +
    labs(x = paste("Error", j))
  
  if (class(t_shape) != "try-error"){
    gg[[j]] <- gg[[j]] +
    geom_line(data = t_errors, aes(x = x, y = y), col = swatch()[6]) +
    geom_label(label = paste("P-Values \n CVM Test:", round(cvm, 2), "\n AD-Test", round(ad, 2)), aes(x = -2, y = 1), size = 3) +
    geom_label(label = paste("SD:", round(sd, 3), "\n DF:", round(df, 2)), aes(x = 2, y = 1), size = 3)
  }
    
}


ggarrange(gg[[1]], gg[[2]], gg[[3]], gg[[4]], gg[[5]], gg[[6]], nrow = 3, ncol = 2) %>% 
  annotate_figure(top = "Absolute Differences from the Average Measurements of the Three Traps for Different Groups",
                  bottom = text_grob("We are looking at absolute differences from the mean fitted with a density kernel estimator (red) and a Student-t distribution (black). \n In the left box we see the bootstrapped P-value for the two goodness-of-fit tests. In the right box we see the sd and degrees of freedom of the fitted t-Distribution.", color = swatch()[1], face = "italic", size = 10))

```


# Some Thoughts

A short review of the methods applied in the draft paper so far. I would like to lay out why the methods used above are probably a preferred choice to compare the traps.

## Correlation vs. Altman-Bland

The Correlation coefficient r is not a reliable metric to compare measurers! see above

## Paired vs. Unpaired

I am pretty sure that paired tests should only be applied if the measurements come from the same trap (i.e. between lines). This is not the case for the trap comparison though!
Paired t-test compares study subjects at 2 different times (paired observations of the same subject). Unpaired t-test (aka Student’s test) compares two different subjects. The paired t-test reduces intersubject variability (because it makes comparisons between the same subject), and thus is theoretically more powerful than the unpaired t-test.

## Pairwise vs. Contrasts

Problem of multiple testing and adjustments of P-Value, the chances of making type one error (reject H0 although true) is actually higher then the alpha chosen.

## Non-parametric vs. parametric

Assumption of linearity is harder to achieve without adding much benefit (Wilcox vs. t-test for simple tests.). It probably makes sense to test apply some robust test in addition to the standard anova t-test that rely on normal distributions.

## linear regression vs. ordinary least products regression analysis 

Geometric mean regression, reduced major axis regression take into account that both y and x contain measurement errors. If measurements have been made on a continuous scale, the main choice is between the Altman–Bland method of differences and least products regression analysis. It is argued that although the former is relatively simple to execute, it does not distinguish adequately between fixed and proportional bias. Least products regression analysis, although more difficult to execute, does achieve this goal. There is almost universal agreement among bio-statisticians that the Pearson product–moment correlation coefficient (r) is valueless as a test for bias.
http://www.jerrydallal.com/LHSP/compare.htm



